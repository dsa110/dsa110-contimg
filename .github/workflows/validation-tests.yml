name: DSA-110 Pipeline Validation Tests

on:
  push:
    branches: [master, master-dev]
  pull_request:
    branches: [master, master-dev]
  schedule:
    # Run tests daily at 02:00 UTC
    - cron: "0 2 * * *"
  workflow_dispatch:
    inputs:
      test_type:
        description: "Type of test to run"
        required: true
        default: "unit"
        type: choice
        options:
          - unit
          - integration
          - validation
          - full

# Prevent duplicate runs on same branch/PR
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  fast-tests:
    name: Fast Tests (Impacted/Fail-Fast)
    runs-on: ubuntu-latest
    timeout-minutes: 20
    env:
      # Use unified database path for CI (Phase 2 consolidation)
      PIPELINE_DB: /tmp/dsa110-ci/pipeline.sqlite3
      # Legacy env vars still supported for backwards compatibility
      PIPELINE_PRODUCTS_DB: /tmp/dsa110-ci/pipeline.sqlite3
      PIPELINE_CAL_REGISTRY_DB: /tmp/dsa110-ci/pipeline.sqlite3
      PIPELINE_HDF5_DB: /tmp/dsa110-ci/pipeline.sqlite3
      PIPELINE_INGEST_DB: /tmp/dsa110-ci/pipeline.sqlite3
      PIPELINE_DATA_REGISTRY_DB: /tmp/dsa110-ci/pipeline.sqlite3
      PIPELINE_DOCSEARCH_DB: /tmp/dsa110-ci/docsearch.sqlite3
      PIPELINE_EMBEDDING_CACHE_DB: /tmp/dsa110-ci/embedding_cache.sqlite3
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.11"

      - name: Create CI database directory
        run: mkdir -p /tmp/dsa110-ci

      - name: Install test dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-xdist pytest-timeout pytest-mock pytest-asyncio httpx
          pip install -e "backend/[dev]"

      - name: Initialize test databases
        env:
          PYTHONPATH: backend/src
        run: |
          python -c "
          from dsa110_contimg.database.session import init_database
          init_database()
          print('Initialized unified pipeline database')
          "

      - name: Run impacted tests (fail-fast)
        env:
          PYTHONPATH: backend/src
        run: |
          set -euo pipefail
          python -m pytest backend/tests/unit/ -v --tb=short -x

  contract-tests:
    name: Contract Tests (Interface Verification)
    runs-on: ubuntu-latest
    timeout-minutes: 30
    env:
      # Use unified database path for CI (Phase 2 consolidation)
      PIPELINE_DB: /tmp/dsa110-ci/pipeline.sqlite3
      # Legacy env vars still supported for backwards compatibility
      PIPELINE_PRODUCTS_DB: /tmp/dsa110-ci/pipeline.sqlite3
      PIPELINE_CAL_REGISTRY_DB: /tmp/dsa110-ci/pipeline.sqlite3
      PIPELINE_HDF5_DB: /tmp/dsa110-ci/pipeline.sqlite3
      PIPELINE_INGEST_DB: /tmp/dsa110-ci/pipeline.sqlite3
      PIPELINE_DATA_REGISTRY_DB: /tmp/dsa110-ci/pipeline.sqlite3
      PIPELINE_DOCSEARCH_DB: /tmp/dsa110-ci/docsearch.sqlite3
      PIPELINE_EMBEDDING_CACHE_DB: /tmp/dsa110-ci/embedding_cache.sqlite3
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.11"

      - name: Create CI database directory
        run: mkdir -p /tmp/dsa110-ci

      - name: Install test dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-xdist pytest-timeout pytest-mock pytest-asyncio httpx
          pip install -e "backend/[dev]"

      - name: Initialize test databases
        env:
          PYTHONPATH: backend/src
        run: |
          python -c "
          from dsa110_contimg.database.session import init_database
          init_database()
          print('Initialized unified pipeline database')
          "

      - name: Run contract tests
        env:
          PYTHONPATH: backend/src
        run: |
          set -euo pipefail
          # Contract tests verify interface behavior without heavy mocking
          python -m pytest backend/tests/contract/ -v \
            --tb=short \
            -W ignore::DeprecationWarning

      - name: Contract test summary
        if: always()
        run: |
          echo "Contract tests verify:"
          echo "  - Conversion: MS structure, antenna positions, data format"
          echo "  - Database: Schema, CRUD operations, migrations"
          echo "  - Imaging: FITS structure, WCS, beam info"
          echo "  - Mosaic: Build, QA, orchestration"
          echo "  - Calibration: Transit, flagging, caltable discovery"
          echo "  - API: Health, status, images, queue endpoints"

  unit-tests:
    name: Unit Tests (Mocked)
    runs-on: ubuntu-latest
    timeout-minutes: 30
    if: ${{ github.event.inputs.test_type == 'unit' || github.event.inputs.test_type == 'full' || github.event.inputs.test_type == '' }}
    env:
      # Use unified database path for CI (Phase 2 consolidation)
      PIPELINE_DB: /tmp/dsa110-ci/pipeline.sqlite3
      # Legacy env vars still supported for backwards compatibility
      PIPELINE_PRODUCTS_DB: /tmp/dsa110-ci/pipeline.sqlite3
      PIPELINE_CAL_REGISTRY_DB: /tmp/dsa110-ci/pipeline.sqlite3
      PIPELINE_HDF5_DB: /tmp/dsa110-ci/pipeline.sqlite3
      PIPELINE_INGEST_DB: /tmp/dsa110-ci/pipeline.sqlite3
      PIPELINE_DATA_REGISTRY_DB: /tmp/dsa110-ci/pipeline.sqlite3
      PIPELINE_DOCSEARCH_DB: /tmp/dsa110-ci/docsearch.sqlite3
      PIPELINE_EMBEDDING_CACHE_DB: /tmp/dsa110-ci/embedding_cache.sqlite3

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.11"

      - name: Create CI database directory
        run: mkdir -p /tmp/dsa110-ci

      - name: Install test dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-mock pytest-asyncio httpx
          pip install -e "backend/[dev]"

      - name: Initialize test databases
        env:
          PYTHONPATH: backend/src
        run: |
          python -c "
          from dsa110_contimg.database.session import init_database
          for db in ['products', 'cal_registry', 'hdf5', 'ingest', 'data_registry']:
              init_database(db)
              print(f'Initialized {db} database')
          "

      - name: Run unit tests with mocking
        env:
          PYTHONPATH: backend/src
        run: |
          set -euo pipefail
          pytest backend/tests/unit/ -v \
            --cov=backend/src/dsa110_contimg \
            --cov-report=xml \
            --cov-report=term-missing \
            --tb=short

      - name: Upload coverage reports
        uses: codecov/codecov-action@v4
        with:
          file: ./coverage.xml
          flags: unittests
          name: unit-tests

  validation-tests:
    name: Validation Tests (Casa6 Environment)
    runs-on: ubuntu-latest
    timeout-minutes: 45
    if: ${{ github.event.inputs.test_type == 'validation' || github.event.inputs.test_type == 'full' }}
    defaults:
      run:
        shell: bash -el {0}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Conda
        uses: conda-incubator/setup-miniconda@v3
        with:
          miniforge-version: latest
          channels: conda-forge
          channel-priority: strict
          activate-environment: casa6
          environment-file: ops/docker/environment.yml
          auto-activate-base: false

      - name: Activate environment and verify installation
        run: |
          set -euo pipefail
          export PYTHONPATH="${GITHUB_WORKSPACE}/backend/src:${PYTHONPATH:-}"

          # Verify conda environment and key packages
          echo "✓ Conda environment activated"
          python --version
          python -c "import casatools; print(f'CASA tools installed')" || echo "⚠ CASA import check skipped"
          python -c "import pyuvdata; print(f'pyuvdata version: {pyuvdata.__version__}')"

          echo "✓ Environment validation complete"
          echo "Note: Comprehensive validation tests run in integration-tests job"

      - name: Install backend dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install -e "./backend"

  integration-tests:
    name: Integration Tests (Synthetic Data)
    runs-on: ubuntu-latest
    timeout-minutes: 60
    if: ${{ github.event.inputs.test_type == 'integration' || github.event.inputs.test_type == 'full' }}
    defaults:
      run:
        shell: bash -el {0}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Conda
        uses: conda-incubator/setup-miniconda@v3
        with:
          miniforge-version: latest
          channels: conda-forge
          channel-priority: strict
          activate-environment: casa6
          environment-file: ops/docker/environment.yml
          auto-activate-base: false

      - name: Install backend dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install -e "./backend"

      - name: Generate synthetic test data
        run: |
          set -euo pipefail
          export PYTHONPATH="${GITHUB_WORKSPACE}/backend/src:${PYTHONPATH:-}"

          # Generate multiple test scenarios
          mkdir -p /tmp/test_scenarios

          # Normal case
          python backend/src/dsa110_contimg/simulation/make_synthetic_uvh5.py \
            --layout-meta backend/src/dsa110_contimg/simulation/config/reference_layout.json \
            --telescope-config backend/src/dsa110_contimg/simulation/pyuvsim/telescope.yaml \
            --output /tmp/test_scenarios/normal \
            --start-time "2025-11-05T14:00:00" \
            --duration-minutes 3.0 \
            --flux-jy 10.0 \
            --subbands 8
            
          # Weak calibrator case
          python backend/src/dsa110_contimg/simulation/make_synthetic_uvh5.py \
            --layout-meta backend/src/dsa110_contimg/simulation/config/reference_layout.json \
            --telescope-config backend/src/dsa110_contimg/simulation/pyuvsim/telescope.yaml \
            --output /tmp/test_scenarios/weak_calibrator \
            --start-time "2025-11-05T15:00:00" \
            --duration-minutes 3.0 \
            --flux-jy 0.5 \
            --subbands 8

      - name: Test pipeline with different scenarios
        run: |
          set -euo pipefail
          export PYTHONPATH="${GITHUB_WORKSPACE}/backend/src:${PYTHONPATH:-}"

          # Test each scenario
          for scenario in normal weak_calibrator; do
            echo "Testing scenario: $scenario"
            
            python -m dsa110_contimg.conversion.strategies.hdf5_orchestrator \
              "/tmp/test_scenarios/$scenario" \
              "/tmp/test_scenarios/${scenario}_ms" \
              "2025-11-05 00:00:00" \
              "2025-11-05 23:59:59"
          done

      - name: Upload integration test results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: integration-test-results
          path: /tmp/test_scenarios/
          retention-days: 7

  performance-tests:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    timeout-minutes: 45
    if: ${{ github.event.inputs.test_type == 'full' }}
    defaults:
      run:
        shell: bash -el {0}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Conda
        uses: conda-incubator/setup-miniconda@v3
        with:
          miniforge-version: latest
          channels: conda-forge
          channel-priority: strict
          activate-environment: casa6
          environment-file: ops/docker/environment.yml
          auto-activate-base: false

      - name: Install backend dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install -e "./backend"

      - name: Run performance benchmarks
        run: |
          set -euo pipefail
          export PYTHONPATH="${GITHUB_WORKSPACE}/backend/src:${PYTHONPATH:-}"

          # Install additional performance testing dependencies
          pip install pytest-benchmark memory-profiler

          # Run performance tests
          pytest benchmarks/ -v \
            --benchmark-only \
            --benchmark-json=/tmp/benchmark.json

      - name: Upload performance results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: performance-benchmarks
          path: /tmp/benchmark.json
          retention-days: 30

  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest
    timeout-minutes: 15
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.11"

      - name: Install quality tools
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install flake8 black isort mypy

      - name: Check code formatting (black)
        continue-on-error: true
        run: black --check --diff backend/src/ backend/tests/

      - name: Check import sorting (isort)
        continue-on-error: true
        run: isort --check-only --diff backend/src/ backend/tests/

      - name: Check code style (flake8)
        continue-on-error: true
        run: flake8 backend/src/ backend/tests/ --max-line-length=88 --extend-ignore=E203,W503

  notify-results:
    name: Notify Test Results
    runs-on: ubuntu-latest
    timeout-minutes: 5
    needs: [unit-tests, validation-tests, integration-tests, code-quality]
    if: always()

    steps:
      - name: Notify success
        if: ${{ (needs.unit-tests.result == 'success' || needs.unit-tests.result == 'skipped') && (needs.validation-tests.result == 'success' || needs.validation-tests.result == 'skipped') && (needs.integration-tests.result == 'success' || needs.integration-tests.result == 'skipped') && (needs.code-quality.result == 'success' || needs.code-quality.result == 'skipped') && needs.unit-tests.result != 'failure' && needs.validation-tests.result != 'failure' && needs.integration-tests.result != 'failure' && needs.code-quality.result != 'failure' }}
        run: |
          echo "✓ All DSA-110 pipeline validation tests passed!"
          echo "The enhanced validation functions are working correctly."

      - name: Notify partial success
        if: ${{ (needs.unit-tests.result == 'success' || needs.unit-tests.result == 'skipped') && needs.unit-tests.result != 'failure' && (needs.validation-tests.result == 'failure' || needs.integration-tests.result == 'failure') }}
        run: |
          echo "⚠ Unit tests passed but validation/integration tests failed."
          echo "This may indicate environment setup issues or real validation problems."

      - name: Notify failure
        if: ${{ needs.unit-tests.result == 'failure' }}
        run: |
          echo "✗ Unit tests failed - this indicates problems with the validation function logic."
          echo "Review the test results and fix validation function implementation."
