name: DSA-110 Pipeline Validation Tests

on:
  push:
    branches: [master]
  pull_request:
    branches: [master]
  schedule:
    # Run tests daily at 02:00 UTC
    - cron: "0 2 * * *"
  workflow_dispatch:
    inputs:
      test_type:
        description: "Type of test to run"
        required: true
        default: "unit"
        type: choice
        options:
          - unit
          - integration
          - validation
          - full

jobs:
  fast-tests:
    name: Fast Tests (Impacted/Fail-Fast)
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.11"

      - name: Install test dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-xdist pytest-timeout pytest-mock
          if [ -f requirements-test.txt ]; then pip install -r requirements-test.txt; fi

      - name: Run impacted tests (fail-fast)
        run: |
          set -euo pipefail
          bash scripts/test-impacted.sh

  unit-tests:
    name: Unit Tests (Mocked)
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.test_type == 'unit' || github.event.inputs.test_type == 'full' || github.event.inputs.test_type == '' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install test dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install -r requirements-test.txt

      - name: Run unit tests with mocking
        run: |
          set -euo pipefail
          export PYTHONPATH="${GITHUB_WORKSPACE}/src:${PYTHONPATH:-}"
          pytest tests/unit/ -v \
            --cov=src \
            --cov-report=xml \
            --cov-report=term-missing \
            --tb=short

      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: unit-tests

  validation-tests:
    name: Validation Tests (Casa6 Environment)
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.test_type == 'validation' || github.event.inputs.test_type == 'full' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Conda
        uses: conda-incubator/setup-miniconda@v3
        with:
          miniconda-version: "latest"
          channels: conda-forge
          channel-priority: strict
          activate-environment: test-env
          python-version: "3.11"

      - name: Activate environment and run validation tests
        shell: bash -l {0}
        run: |
          set -euo pipefail
          conda activate test-env
          export PYTHONPATH="${GITHUB_WORKSPACE}/src:${PYTHONPATH:-}"

          # Install test dependencies
          pip install -r requirements-test.txt

          # Run validation tests
          pytest tests/unit/ -v --tb=short || echo "Validation tests completed with issues"

      - name: Upload validation test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: validation-test-results
          path: /tmp/dsa110_validation_test_*/
          retention-days: 7

  integration-tests:
    name: Integration Tests (Synthetic Data)
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.test_type == 'integration' || github.event.inputs.test_type == 'full' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Conda
        uses: conda-incubator/setup-miniconda@v3
        with:
          miniconda-version: "latest"
          channels: conda-forge
          channel-priority: strict
          activate-environment: test-env
          python-version: "3.11"

      - name: Generate synthetic test data
        shell: bash -l {0}
        run: |
          set -euo pipefail
          conda activate test-env
          export PYTHONPATH="${GITHUB_WORKSPACE}/src:${PYTHONPATH:-}"

          # Install test dependencies
          pip install -r requirements-test.txt

          # Run integration tests
          pytest tests/integration/ -v --tb=short || echo "Integration tests completed with issues"

      - name: Upload integration test results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: integration-test-results
          path: /tmp/test_scenarios/
          retention-days: 7

  performance-tests:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.test_type == 'full' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Conda
        uses: conda-incubator/setup-miniconda@v3
        with:
          miniconda-version: "latest"
          channels: conda-forge
          channel-priority: strict
          activate-environment: test-env
          python-version: "3.11"

      - name: Run performance benchmarks
        shell: bash -l {0}
        run: |
          set -euo pipefail
          conda activate test-env
          export PYTHONPATH="${GITHUB_WORKSPACE}/src:${PYTHONPATH:-}"

          # Install test and benchmark dependencies
          pip install -r requirements-test.txt
          pip install pytest-benchmark memory-profiler

          # Run performance tests if they exist
          if [ -d "tests/performance" ]; then
            pytest tests/performance/ -v \
              --benchmark-only \
              --benchmark-json=/tmp/benchmark.json \
              || echo "Performance tests completed with issues"
          else
            echo "No performance tests directory found, skipping"
          fi

      - name: Upload performance results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: performance-benchmarks
          path: /tmp/benchmark.json
          retention-days: 30

  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"

      - name: Install quality tools
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install flake8 black isort mypy

      - name: Run code quality checks
        run: |
          set -euo pipefail
          # Check code formatting
          black --check --diff src/ tests/

          # Check import sorting
          isort --check-only --diff src/ tests/

          # Check code style
          flake8 src/ tests/ --max-line-length=79 --extend-ignore=E203,W503

          # Type checking (optional - may have many issues with CASA dependencies)
          # mypy src/dsa110_contimg/ || echo "Type checking found issues - review recommended"

  notify-results:
    name: Notify Test Results
    runs-on: ubuntu-latest
    needs: [unit-tests, validation-tests, integration-tests, code-quality]
    if: always()

    steps:
      - name: Notify success
        if: ${{ needs.unit-tests.result == 'success' && needs.validation-tests.result == 'success' && needs.integration-tests.result == 'success' && needs.code-quality.result == 'success' }}
        run: |
          echo "✅ All DSA-110 pipeline validation tests passed!"
          echo "The enhanced validation functions are working correctly."

      - name: Notify partial success
        if: ${{ needs.unit-tests.result == 'success' && (needs.validation-tests.result == 'failure' || needs.integration-tests.result == 'failure') }}
        run: |
          echo "⚠️ Unit tests passed but validation/integration tests failed."
          echo "This may indicate environment setup issues or real validation problems."

      - name: Notify failure
        if: ${{ needs.unit-tests.result == 'failure' }}
        run: |
          echo "❌ Unit tests failed - this indicates problems with the validation function logic."
          echo "Review the test results and fix validation function implementation."
