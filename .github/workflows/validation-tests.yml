name: DSA-110 Pipeline Validation Tests

on:
  push:
    branches: [main, dev]
  pull_request:
    branches: [main, dev]
  schedule:
    # Run tests daily at 02:00 UTC
    - cron: "0 2 * * *"
  workflow_dispatch:
    inputs:
      test_type:
        description: "Type of test to run"
        required: true
        default: "unit"
        type: choice
        options:
          - unit
          - integration
          - validation
          - full

jobs:
  fast-tests:
    name: Fast Tests (Impacted/Fail-Fast)
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.11"

      - name: Install test dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install pytest pytest-cov pytest-xdist pytest-timeout pytest-mock
          pip install -e "backend/[dev]"

      - name: Run impacted tests (fail-fast)
        env:
          PYTHONPATH: backend/src
          PIPELINE_PRODUCTS_DB: /tmp/dsa110-ci/products.sqlite3
          PIPELINE_CAL_REGISTRY_DB: /tmp/dsa110-ci/cal_registry.sqlite3
          PIPELINE_HDF5_DB: /tmp/dsa110-ci/hdf5.sqlite3
          PIPELINE_INGEST_DB: /tmp/dsa110-ci/ingest.sqlite3
          PIPELINE_DATA_REGISTRY_DB: /tmp/dsa110-ci/data_registry.sqlite3
          PIPELINE_DOCSEARCH_DB: /tmp/dsa110-ci/docsearch.sqlite3
          PIPELINE_EMBEDDING_CACHE_DB: /tmp/dsa110-ci/embedding_cache.sqlite3
        run: |
          set -euo pipefail
          mkdir -p /tmp/dsa110-ci
          pytest backend/tests/unit/ -v -x --maxfail=1

  unit-tests:
    name: Unit Tests (Mocked)
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.test_type == 'unit' || github.event.inputs.test_type == 'full' || github.event.inputs.test_type == '' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.11"

      - name: Install test dependencies
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install -e "backend/[dev]"

      - name: Run unit tests with mocking
        env:
          PYTHONPATH: backend/src
          PIPELINE_PRODUCTS_DB: /tmp/dsa110-ci/products.sqlite3
          PIPELINE_CAL_REGISTRY_DB: /tmp/dsa110-ci/cal_registry.sqlite3
          PIPELINE_HDF5_DB: /tmp/dsa110-ci/hdf5.sqlite3
          PIPELINE_INGEST_DB: /tmp/dsa110-ci/ingest.sqlite3
          PIPELINE_DATA_REGISTRY_DB: /tmp/dsa110-ci/data_registry.sqlite3
          PIPELINE_DOCSEARCH_DB: /tmp/dsa110-ci/docsearch.sqlite3
          PIPELINE_EMBEDDING_CACHE_DB: /tmp/dsa110-ci/embedding_cache.sqlite3
        run: |
          set -euo pipefail
          mkdir -p /tmp/dsa110-ci
          pytest backend/tests/unit/ -v \
            --cov=backend/src/dsa110_contimg \
            --cov-report=xml \
            --cov-report=term-missing \
            --tb=short

      - name: Upload coverage reports
        uses: codecov/codecov-action@v3
        with:
          file: ./coverage.xml
          flags: unittests
          name: unit-tests

  validation-tests:
    name: Validation Tests (Casa6 Environment)
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.test_type == 'validation' || github.event.inputs.test_type == 'full' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Conda
        uses: conda-incubator/setup-miniconda@v2
        with:
          miniconda-version: "latest"
          channels: conda-forge
          channel-priority: strict
          activate-environment: casa6
          environment-file: ops/docker/environment.yml

      - name: Install backend dependencies
        shell: bash -l {0}
        run: |
          set -euo pipefail
          conda activate casa6
          pip install -e "./backend"

      - name: Activate environment and run validation tests
        shell: bash -l {0}
        run: |
          set -euo pipefail
          conda activate casa6
          export PYTHONPATH="${GITHUB_WORKSPACE}/src:${PYTHONPATH:-}"

          # Run validation test script
          # TODO: Implement test_enhanced_pipeline_production.sh or remove this step
          # chmod +x test_enhanced_pipeline_production.sh
          # ./test_enhanced_pipeline_production.sh

          echo "⚠️ Validation test script not yet implemented"
          echo "This step is currently a placeholder and will be implemented in the future"

      - name: Upload validation test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: validation-test-results
          path: /tmp/dsa110_validation_test_*/
          retention-days: 7

  integration-tests:
    name: Integration Tests (Synthetic Data)
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.test_type == 'integration' || github.event.inputs.test_type == 'full' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Conda
        uses: conda-incubator/setup-miniconda@v2
        with:
          miniconda-version: "latest"
          channels: conda-forge
          channel-priority: strict
          activate-environment: casa6
          environment-file: ops/docker/environment.yml

      - name: Install backend dependencies
        shell: bash -l {0}
        run: |
          set -euo pipefail
          conda activate casa6
          pip install -e "./backend"

      - name: Generate synthetic test data
        shell: bash -l {0}
        run: |
          set -euo pipefail
          conda activate casa6
          export PYTHONPATH="${GITHUB_WORKSPACE}/src:${PYTHONPATH:-}"

          # Generate multiple test scenarios
          mkdir -p /tmp/test_scenarios

          # Normal case
          python src/dsa110_contimg/simulation/make_synthetic_uvh5.py \
            --layout-meta src/dsa110_contimg/simulation/config/reference_layout.json \
            --telescope-config src/dsa110_contimg/simulation/pyuvsim/telescope.yaml \
            --output /tmp/test_scenarios/normal \
            --start-time "2025-11-05T14:00:00" \
            --duration-minutes 3.0 \
            --flux-jy 10.0 \
            --subbands 8
            
          # Weak calibrator case
          python src/dsa110_contimg/simulation/make_synthetic_uvh5.py \
            --layout-meta src/dsa110_contimg/simulation/config/reference_layout.json \
            --telescope-config src/dsa110_contimg/simulation/pyuvsim/telescope.yaml \
            --output /tmp/test_scenarios/weak_calibrator \
            --start-time "2025-11-05T15:00:00" \
            --duration-minutes 3.0 \
            --flux-jy 0.5 \
            --subbands 8

      - name: Test pipeline with different scenarios
        shell: bash -l {0}
        run: |
          set -euo pipefail
          conda activate casa6
          export PYTHONPATH="${GITHUB_WORKSPACE}/src:${PYTHONPATH:-}"

          # Test each scenario
          for scenario in normal weak_calibrator; do
            echo "Testing scenario: $scenario"
            
            python -m dsa110_contimg.conversion.strategies.hdf5_orchestrator \
              "/tmp/test_scenarios/$scenario" \
              "/tmp/test_scenarios/${scenario}_ms" \
              "2025-11-05 00:00:00" \
              "2025-11-05 23:59:59" \
              || echo "Scenario $scenario failed as expected"
          done

      - name: Upload integration test results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: integration-test-results
          path: /tmp/test_scenarios/
          retention-days: 7

  performance-tests:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.test_type == 'full' }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Conda
        uses: conda-incubator/setup-miniconda@v2
        with:
          miniconda-version: "latest"
          channels: conda-forge
          channel-priority: strict
          activate-environment: casa6
          environment-file: ops/docker/environment.yml

      - name: Install backend dependencies
        shell: bash -l {0}
        run: |
          set -euo pipefail
          conda activate casa6
          pip install -e "./backend"

      - name: Run performance benchmarks
        shell: bash -l {0}
        run: |
          set -euo pipefail
          conda activate casa6
          export PYTHONPATH="${GITHUB_WORKSPACE}/src:${PYTHONPATH:-}"

          # Install additional performance testing dependencies
          pip install pytest-benchmark memory-profiler

          # Run performance tests
          pytest tests/performance/ -v \
            --benchmark-only \
            --benchmark-json=/tmp/benchmark.json \
            || echo "Performance tests failed - reviewing results"

      - name: Upload performance results
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: performance-benchmarks
          path: /tmp/benchmark.json
          retention-days: 30

  code-quality:
    name: Code Quality Checks
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v6
        with:
          python-version: "3.11"

      - name: Install quality tools
        run: |
          set -euo pipefail
          python -m pip install --upgrade pip
          pip install flake8 black isort mypy

      - name: Run code quality checks
        run: |
          set -euo pipefail
          # Check code formatting
          black --check --diff backend/src/ backend/tests/

          # Check import sorting
          isort --check-only --diff backend/src/ backend/tests/

          # Check code style
          flake8 backend/src/ backend/tests/ --max-line-length=79 --extend-ignore=E203,W503

          # Type checking (optional - may have many issues with CASA dependencies)
          # mypy src/dsa110_contimg/ || echo "Type checking found issues - review recommended"

  notify-results:
    name: Notify Test Results
    runs-on: ubuntu-latest
    needs: [unit-tests, validation-tests, integration-tests, code-quality]
    if: always()

    steps:
      - name: Notify success
        if: ${{ (needs.unit-tests.result == 'success' || needs.unit-tests.result == 'skipped') && (needs.validation-tests.result == 'success' || needs.validation-tests.result == 'skipped') && (needs.integration-tests.result == 'success' || needs.integration-tests.result == 'skipped') && (needs.code-quality.result == 'success' || needs.code-quality.result == 'skipped') && needs.unit-tests.result != 'failure' && needs.validation-tests.result != 'failure' && needs.integration-tests.result != 'failure' && needs.code-quality.result != 'failure' }}
        run: |
          echo "✅ All DSA-110 pipeline validation tests passed!"
          echo "The enhanced validation functions are working correctly."

      - name: Notify partial success
        if: ${{ (needs.unit-tests.result == 'success' || needs.unit-tests.result == 'skipped') && needs.unit-tests.result != 'failure' && (needs.validation-tests.result == 'failure' || needs.integration-tests.result == 'failure') }}
        run: |
          echo "⚠️ Unit tests passed but validation/integration tests failed."
          echo "This may indicate environment setup issues or real validation problems."

      - name: Notify failure
        if: ${{ needs.unit-tests.result == 'failure' }}
        run: |
          echo "❌ Unit tests failed - this indicates problems with the validation function logic."
          echo "Review the test results and fix validation function implementation."
