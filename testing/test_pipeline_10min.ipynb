{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a5753f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "# Notebook Setup Cell\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from importlib import reload\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from collections import defaultdict\n",
    "from pickleshare import *\n",
    "\n",
    "# Astropy imports\n",
    "from astropy.time import Time, TimeDelta\n",
    "from astropy.coordinates import SkyCoord, Angle, EarthLocation\n",
    "import astropy.units as u\n",
    "from astropy.table import Table\n",
    "from astropy.io import fits\n",
    "from astropy.wcs import WCS\n",
    "\n",
    "# --- IMPORTANT: Adjust sys.path if needed ---\n",
    "# If your notebook is NOT in the same directory as the 'pipeline' folder,\n",
    "# add the parent directory to the path so Python can find the modules.\n",
    "pipeline_parent_dir = '/data/jfaber/dsa110-contimg/' # ADJUST IF YOUR NOTEBOOK IS ELSEWHERE\n",
    "if pipeline_parent_dir not in sys.path:\n",
    "    sys.path.insert(0, pipeline_parent_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99032803",
   "metadata": {},
   "outputs": [],
   "source": [
    "from casatasks import (\n",
    "        clearcal, delmod, rmtables, flagdata, bandpass, ft, mstransform, gaincal, applycal, listobs, split\n",
    "    )\n",
    "from casatools import componentlist, msmetadata, imager, ms, table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80d8c577",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline module imports\n",
    "try:\n",
    "    from pipeline import config_parser\n",
    "    from pipeline import pipeline_utils\n",
    "    from pipeline import ms_creation\n",
    "    from pipeline import calibration\n",
    "    from pipeline import skymodel\n",
    "    from pipeline import imaging\n",
    "    from pipeline import mosaicking\n",
    "    from pipeline import photometry\n",
    "    from pipeline import dsa110_utils # Needed for location\n",
    "except ImportError as e:\n",
    "    print(f\"ERROR: Failed to import pipeline modules. Check sys.path.\")\n",
    "    print(f\"Current sys.path: {sys.path}\")\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b658bb34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'pipeline.ms_creation' from '/data/jfaber/dsa110-contimg/pipeline/ms_creation.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(ms_creation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a30c76d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-12 08:08:32 [INFO ] [MainThread] [root] CASA log file set to: /data/jfaber/logs/casa_20250512_080832.log\n",
      "2025-05-12 08:08:32 [INFO ] [MainThread] [root] Pipeline logging configured. Log file: /data/jfaber/logs/notebook_test_080832_20250512_080832.log\n",
      "2025-05-12 08:08:32 [INFO ] [MainThread] [root] Setup cell executed.\n"
     ]
    }
   ],
   "source": [
    "# pyuvdata needed for reading header\n",
    "try:\n",
    "    from pyuvdata import UVData\n",
    "    pyuvdata_available = True\n",
    "except ImportError:\n",
    "     print(\"ERROR: pyuvdata is required to read HDF5 metadata.\")\n",
    "     pyuvdata_available = False # Script will likely fail later\n",
    "\n",
    "# --- Define Paths and Parameters (modify as needed) ---\n",
    "CONFIG_PATH = 'config/pipeline_config.yaml' # Relative path from notebook location\n",
    "HDF5_DIR = '/data/incoming/' # Location of your HDF5 data chunks\n",
    "BCAL_NAME_OVERRIDE = None # Optional: Force a specific BPCAL name for testing, e.g., '3C286', otherwise set to None\n",
    "VERBOSE_LOGGING = True # Set True for DEBUG level, False for INFO\n",
    "\n",
    "# --- Setup Logging ---\n",
    "# Load config minimally just to get log path\n",
    "try:\n",
    "    with open(CONFIG_PATH, 'r') as f:\n",
    "        temp_config_for_log = yaml.safe_load(f)\n",
    "    log_dir_config = temp_config_for_log.get('paths', {}).get('log_dir', 'logs') #\n",
    "\n",
    "    # Resolve log_dir relative to pipeline parent dir if log_dir_config is relative\n",
    "    if not os.path.isabs(log_dir_config):\n",
    "        # Assumes pipeline_utils.py is in 'pipeline_parent_dir/pipeline/'\n",
    "        # and log_dir in config is relative to 'pipeline_parent_dir'\n",
    "        # Example: config log_dir: ../logs -> resolved: pipeline_parent_dir/../logs\n",
    "        # If log_dir is like 'logs/', it will be pipeline_parent_dir/logs/\n",
    "        # The config_parser.py resolves log_dir relative to the parent of the script dir.\n",
    "        # For consistency, let's assume pipeline_parent_dir is the project root.\n",
    "        log_dir = os.path.abspath(os.path.join(pipeline_parent_dir, log_dir_config))\n",
    "    else:\n",
    "        log_dir = log_dir_config\n",
    "\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    log_level = logging.DEBUG if VERBOSE_LOGGING else logging.INFO\n",
    "    \n",
    "    # Ensure CASA log is also set if casatasks is available\n",
    "    logger = pipeline_utils.setup_logging(log_dir, config_name=f\"notebook_test_{datetime.now().strftime('%H%M%S')}\") #\n",
    "    logger.setLevel(log_level)\n",
    "    \n",
    "    # Suppress overly verbose CASA logs if desired (from casatasks import casalog; casalog.filter('INFO'))\n",
    "    logger.info(\"Setup cell executed.\")\n",
    "except Exception as e:\n",
    "    print(f\"ERROR during setup: {e}\")\n",
    "    # Stop execution if setup fails\n",
    "    raise RuntimeError(\"Setup failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d16b9f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-12 08:08:36 [INFO ] [MainThread] [root] Helper functions defined.\n"
     ]
    }
   ],
   "source": [
    "# Notebook Cell: Helper Function Definitions\n",
    "\n",
    "from collections import defaultdict \n",
    "\n",
    "def collect_files_for_nominal_start_time(nominal_start_time_str, hdf5_dir, config):\n",
    "    \"\"\"\n",
    "    Collects a complete set of HDF5 files for a nominal start time,\n",
    "    respecting timestamp variations via same_timestamp_tolerance.\n",
    "    Handles HDF5 filenames with timestamps like 'YYYY-MM-DDTHH:MM:SS_sbXX.hdf5'.\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(__name__)\n",
    "    \n",
    "    # Format for the user-provided nominal start time string\n",
    "    nominal_time_format = \"%Y%m%dT%H%M%S\" \n",
    "    # Format for timestamps found in the actual HDF5 filenames\n",
    "    actual_file_time_format = \"%Y-%m-%dT%H:%M:%S\" # Corrected format\n",
    "\n",
    "    try:\n",
    "        # Parse the user-provided nominal start time\n",
    "        nominal_dt_obj = datetime.strptime(nominal_start_time_str, nominal_time_format)\n",
    "    except ValueError:\n",
    "        logger.error(f\"Invalid nominal_start_time_str format: {nominal_start_time_str}. Expected {nominal_time_format}.\")\n",
    "        return None\n",
    "\n",
    "    tolerance_sec = config['ms_creation'].get('same_timestamp_tolerance', 30.0)\n",
    "    expected_spws_set = set(config['ms_creation']['spws'])\n",
    "    \n",
    "    logger.info(f\"Collecting files for nominal start time: {nominal_start_time_str} (parsed as {nominal_dt_obj}) in {hdf5_dir} with tolerance {tolerance_sec}s\")\n",
    "    logger.debug(f\"Expected SPWs: {sorted(list(expected_spws_set))}\")\n",
    "\n",
    "    files_for_this_chunk = defaultdict(list)\n",
    "    all_hdf5_files_in_dir = glob.glob(os.path.join(hdf5_dir, \"20*.hdf5\")) # Glob for files starting with \"20\"\n",
    "    logger.debug(f\"Found {len(all_hdf5_files_in_dir)} total HDF5 files in {hdf5_dir} to check.\")\n",
    "\n",
    "    found_any_for_nominal_time = False\n",
    "    for f_path in all_hdf5_files_in_dir:\n",
    "        try:\n",
    "            f_name = os.path.basename(f_path)\n",
    "            # Assuming filename format YYYY-MM-DDTHH:MM:SS_sbXX.hdf5\n",
    "            ts_str_from_file = f_name.split('_')[0] \n",
    "            \n",
    "            # Parse timestamp from the filename using the correct format\n",
    "            file_dt_obj = datetime.strptime(ts_str_from_file, actual_file_time_format)\n",
    "            \n",
    "            time_diff_seconds = abs((file_dt_obj - nominal_dt_obj).total_seconds())\n",
    "            \n",
    "            if time_diff_seconds <= tolerance_sec:\n",
    "                found_any_for_nominal_time = True\n",
    "                spw_str_from_file = f_name.split('_')[1].replace('.hdf5', '')\n",
    "                base_spw = spw_str_from_file # Since 'spl' is no longer used\n",
    "                \n",
    "                logger.debug(f\"  File {f_name}: ActualTS={file_dt_obj}, time_diff={time_diff_seconds:.1f}s, parsed_spw='{base_spw}'\")\n",
    "                if base_spw in expected_spws_set:\n",
    "                    files_for_this_chunk[base_spw].append(f_path)\n",
    "                    logger.debug(f\"    -> Matched expected SPW: '{base_spw}'\")\n",
    "                else:\n",
    "                    logger.debug(f\"    -> Parsed SPW '{base_spw}' not in expected_spws_set.\")\n",
    "            # else: # This else can be very verbose if many files are outside the tolerance\n",
    "                # logger.debug(f\"  File {f_name}: ActualTS={file_dt_obj}, time_diff={time_diff_seconds:.1f}s (OUTSIDE tolerance for {nominal_start_time_str})\")\n",
    "\n",
    "        except (IndexError, ValueError) as e_parse: # Catch errors from split or strptime\n",
    "            logger.debug(f\"Could not parse filename or timestamp for {f_name} (format expected: YYYY-MM-DDTHH:MM:SS_sbXX.hdf5): {e_parse}\")\n",
    "            continue\n",
    "        except Exception as e_gen: # Catch any other unexpected errors for a file\n",
    "            logger.debug(f\"Unexpected error processing file {f_name}: {e_gen}\")\n",
    "            continue\n",
    "            \n",
    "    if not found_any_for_nominal_time:\n",
    "        logger.warning(f\"No HDF5 files found whose timestamps were within the {tolerance_sec}s tolerance window for nominal start time {nominal_start_time_str} ({nominal_dt_obj}).\")\n",
    "\n",
    "    collected_files_list = []\n",
    "    is_complete = True\n",
    "    missing_spws = []\n",
    "    for spw_needed in sorted(list(expected_spws_set)): \n",
    "        if spw_needed in files_for_this_chunk and files_for_this_chunk[spw_needed]:\n",
    "            files_for_this_chunk[spw_needed].sort() \n",
    "            collected_files_list.append(files_for_this_chunk[spw_needed][0]) \n",
    "        else:\n",
    "            is_complete = False\n",
    "            missing_spws.append(spw_needed)\n",
    "\n",
    "    if not is_complete:\n",
    "        logger.error(f\"Incomplete HDF5 set for nominal time {nominal_start_time_str}: Missing SPW(s): {', '.join(missing_spws)}\")\n",
    "        return None # Return None if set is not complete\n",
    "            \n",
    "    if len(collected_files_list) == len(expected_spws_set): # Check if all expected SPWs were collected\n",
    "        logger.info(f\"Found complete set of {len(collected_files_list)} files for nominal start time {nominal_start_time_str}\")\n",
    "        return sorted(collected_files_list) \n",
    "    else:\n",
    "        # This path should ideally be caught by \"is_complete\" check, but good for robustness\n",
    "        logger.error(f\"Failed to form a complete set for nominal start {nominal_start_time_str}. Expected {len(expected_spws_set)}, collected {len(collected_files_list)}. Missing: {', '.join(missing_spws)}\")\n",
    "        return None\n",
    "\n",
    "        \n",
    "def get_obs_declination(config, hdf5_dir):\n",
    "    \"\"\"Reads the fixed declination from an arbitrary HDF5 file's metadata.\"\"\"\n",
    "    if not pyuvdata_available: return None\n",
    "    logging.info(\"Attempting to determine observation declination from HDF5 metadata...\")\n",
    "    try:\n",
    "        pattern = os.path.join(hdf5_dir, \"20*_sb00.hdf5\")\n",
    "        hdf5_files = glob.glob(pattern)\n",
    "        if not hdf5_files:\n",
    "            raise FileNotFoundError(f\"No '*_sb00.hdf5' files found in {hdf5_dir} to read metadata.\")\n",
    "        uvd = UVData()\n",
    "        logging.debug(f\"Reading metadata from: {hdf5_files[0]}\")\n",
    "        uvd.read(hdf5_files[0], file_type='uvh5', run_check=False, read_data=False)\n",
    "        fixed_dec_rad = uvd.extra_keywords['phase_center_dec']\n",
    "        fixed_dec_deg = np.rad2deg(fixed_dec_rad) % 360.0\n",
    "        logging.info(f\"Determined observation Declination: {fixed_dec_deg:.4f} degrees\")\n",
    "        return fixed_dec_deg\n",
    "    except KeyError:\n",
    "        logging.error(f\"Metadata key 'phase_center_dec' not found in {hdf5_files[0]}. Cannot determine Dec.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to read HDF5 metadata to determine Declination: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def select_bcal_for_test(config, fixed_dec_deg, bcal_name_override=None):\n",
    "    \"\"\"Reads BPCAL candidate catalog, filters by Dec, selects one for testing.\"\"\"\n",
    "    logger = pipeline_utils.get_logger(__name__)\n",
    "    cal_config = config['calibration']\n",
    "    # Path to the *filtered* candidate list\n",
    "    bcal_catalog_path = cal_config['bcal_candidate_catalog'] # Assumes path is resolved\n",
    "    # Use flux limits from config if not overridden for selection\n",
    "    min_flux_jy = cal_config.get('bcal_min_flux_jy', 1.0)\n",
    "    max_flux_jy = cal_config.get('bcal_max_flux_jy', 100.0)\n",
    "\n",
    "    if not os.path.exists(bcal_catalog_path):\n",
    "        logger.error(f\"BPCAL candidate catalog not found: {bcal_catalog_path}\")\n",
    "        logger.error(\"Please run the catalog generation script first (e.g., filter_vla_catalog_for_bcal.py).\")\n",
    "        return None\n",
    "\n",
    "    logger.info(f\"Reading BPCAL candidates from: {bcal_catalog_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(bcal_catalog_path, na_values=['None','NaN',''])\n",
    "        if df.empty:\n",
    "            logger.error(f\"BPCAL candidate file '{bcal_catalog_path}' is empty.\")\n",
    "            logger.error(f\"Check filtering criteria (Dec={fixed_dec_deg:.2f}, Flux={min_flux_jy}-{max_flux_jy}Jy) or generate the file.\")\n",
    "            return None\n",
    "\n",
    "        # In case the loaded file wasn't generated for this exact Dec, filter again.\n",
    "        beam_radius_deg = cal_config.get('bcal_search_beam_radius_deg', 1.5)\n",
    "        dec_min = fixed_dec_deg - beam_radius_deg\n",
    "        dec_max = fixed_dec_deg + beam_radius_deg\n",
    "        df['dec_deg'] = df['dec_str'].apply(lambda x: Angle(x.replace('\"',''), unit=u.deg).deg if pd.notna(x) else np.nan)\n",
    "        dec_mask = (df['dec_deg'] >= dec_min) & (df['dec_deg'] <= dec_max) & (df['dec_deg'].notna())\n",
    "        df_filtered = df[dec_mask].copy() # Use copy to avoid SettingWithCopyWarning later\n",
    "\n",
    "        if df_filtered.empty:\n",
    "             logger.error(f\"No BPCAL candidates found in '{bcal_catalog_path}' within Dec range [{dec_min:.2f}, {dec_max:.2f}] deg.\")\n",
    "             return None\n",
    "\n",
    "        # Ensure flux is numeric\n",
    "        df_filtered['flux_num'] = pd.to_numeric(df_filtered['flux_jy'], errors='coerce')\n",
    "        df_filtered = df_filtered.dropna(subset=['flux_num'])\n",
    "\n",
    "        if df_filtered.empty:\n",
    "             logger.error(f\"No BPCAL candidates remain after converting flux to numeric.\")\n",
    "             return None\n",
    "\n",
    "        # Select calibrator\n",
    "        selected_cal = None\n",
    "        if bcal_name_override:\n",
    "            logger.info(f\"Attempting to use specified BPCAL: {bcal_name_override}\")\n",
    "            selection = df_filtered[df_filtered['name'] == bcal_name_override]\n",
    "            if not selection.empty:\n",
    "                selected_cal = selection.iloc[0]\n",
    "            else:\n",
    "                logger.error(f\"Specified BPCAL '{bcal_name_override}' not found in filtered list.\")\n",
    "                return None\n",
    "        else:\n",
    "            # Select the brightest one in the filtered list\n",
    "            selected_cal = df_filtered.loc[df_filtered['flux_num'].idxmax()]\n",
    "            logger.info(f\"Selected brightest available BPCAL: {selected_cal['name']} (L-Flux: {selected_cal['flux_num']:.2f} Jy)\")\n",
    "\n",
    "        # Return info as a dictionary matching format needed by skymodel.create_calibrator_component_list\n",
    "        cal_info = {\n",
    "            'name': selected_cal['name'],\n",
    "            'ra': selected_cal['ra_str'],\n",
    "            'dec': selected_cal['dec_str'],\n",
    "            'epoch': selected_cal.get('epoch', 'J2000'), # Default epoch if missing\n",
    "            'flux_jy': selected_cal['flux_num'],\n",
    "            'ref_freq_ghz': 1.4, # Assume L-band flux reference\n",
    "            'spectral_index': None # Not available\n",
    "        }\n",
    "        return cal_info\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to read or select from BPCAL catalog '{bcal_catalog_path}': {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def calculate_next_transit(bcal_info, telescope_loc):\n",
    "    \"\"\"Calculates the next transit time for the selected calibrator.\"\"\"\n",
    "    logger = pipeline_utils.get_logger(__name__)\n",
    "    try:\n",
    "        # Use SkyCoord for robust parsing and calculations\n",
    "        cal_coord = SkyCoord(ra=bcal_info['ra'], dec=bcal_info['dec'], unit=(u.hourangle, u.deg), frame='icrs')\n",
    "        logger.debug(f\"BPCAL Coordinate: {cal_coord.to_string('hmsdms')}\")\n",
    "\n",
    "        current_time_utc = Time.now()\n",
    "        # Calculate LST at current time\n",
    "        current_lst = current_time_utc.sidereal_time('apparent', longitude=telescope_loc.lon)\n",
    "        # Calculate HA of source now\n",
    "        current_ha = (current_lst - cal_coord.ra).wrap_at(180 * u.deg)\n",
    "        # Time until next transit (when HA = 0) is -HA / (rate of change of HA = Earth rotation rate)\n",
    "        earth_rot_rate_approx = 360.9856 * u.deg / u.day # More precise rate\n",
    "        time_to_transit = -current_ha / earth_rot_rate_approx\n",
    "\n",
    "        next_transit_time = current_time_utc + time_to_transit\n",
    "\n",
    "        # If time_to_transit is negative, it means transit already happened today,\n",
    "        # so add one sidereal day (approx) to get the *next* one.\n",
    "        if time_to_transit < TimeDelta(0 * u.s):\n",
    "            next_transit_time += TimeDelta(1.0, format='jd', scale='tdb') * (1 * u.sday).to(u.day) # Add approx 1 sidereal day\n",
    "\n",
    "        logger.info(f\"Calculated next transit for {bcal_info['name']} at: {next_transit_time.iso}\")\n",
    "        return next_transit_time\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to calculate transit time for {bcal_info['name']}: {e}\", exc_info=True)\n",
    "        return None\n",
    "\n",
    "def find_hdf5_chunks_around_time(config, hdf5_dir, target_time):\n",
    "    \"\"\"Finds the HDF5 sets for the 5-min chunk containing target_time and the one before it.\"\"\"\n",
    "    logger = pipeline_utils.get_logger(__name__)\n",
    "    ms_chunk_mins = config['services'].get('ms_chunk_duration_min', 5)\n",
    "    tolerance_sec = config['ms_creation'].get('same_timestamp_tolerance', 30) # Use tolerance\n",
    "\n",
    "    logger.info(f\"Searching for HDF5 chunks around target transit time: {target_time.iso}\")\n",
    "\n",
    "    # Find all potential start times from filenames\n",
    "    all_files = glob.glob(os.path.join(hdf5_dir, \"20*_sb00.hdf5\"))\n",
    "    if not all_files:\n",
    "         logger.error(f\"No HDF5 files found in {hdf5_dir} matching pattern.\")\n",
    "         return None, None, None, None\n",
    "\n",
    "    possible_start_times = []\n",
    "    time_format = \"%Y%m%dT%H%M%S\"\n",
    "    for f in all_files:\n",
    "        try:\n",
    "            ts_str = os.path.basename(f).split('_')[0]\n",
    "            t = Time(datetime.strptime(ts_str, time_format), format='datetime', scale='utc')\n",
    "            possible_start_times.append(t)\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Could not parse time from {os.path.basename(f)}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not possible_start_times:\n",
    "        logger.error(f\"No valid timestamps parsed from HDF5 files found in {hdf5_dir}.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    possible_start_times = sorted(list(set(possible_start_times))) # Unique sorted times\n",
    "    logger.debug(f\"Found {len(possible_start_times)} unique potential start times.\")\n",
    "\n",
    "    # Find the chunk containing the target_time (transit)\n",
    "    transit_chunk_start_time = None\n",
    "    for i, t_start in enumerate(possible_start_times):\n",
    "        # Consider a chunk valid if the target time is within +/- tolerance/2 of its midpoint?\n",
    "        # Or simpler: find chunk where target_time falls between t_start and t_start + chunk_duration\n",
    "        t_end = t_start + timedelta(minutes=ms_chunk_mins)\n",
    "        if t_start <= target_time < t_end:\n",
    "            transit_chunk_start_time = t_start\n",
    "            logger.info(f\"Found transit chunk starting at: {transit_chunk_start_time.iso}\")\n",
    "            break\n",
    "\n",
    "    # Handle case where target time is not exactly within a chunk (pick closest start time before it)\n",
    "    if transit_chunk_start_time is None:\n",
    "         times_before = [t for t in possible_start_times if t <= target_time]\n",
    "         if not times_before:\n",
    "              logger.error(f\"No HDF5 chunks found starting at or before the target transit time {target_time.iso}.\")\n",
    "              return None, None, None, None\n",
    "         transit_chunk_start_time = times_before[-1] # Closest start time <= target time\n",
    "         logger.warning(f\"Target time {target_time.iso} not within a chunk's exact 5min window. Selecting closest preceding chunk: {transit_chunk_start_time.iso}\")\n",
    "\n",
    "\n",
    "    # Find the preceding chunk\n",
    "    preceding_chunk_start_time = None\n",
    "    transit_chunk_index = possible_start_times.index(transit_chunk_start_time)\n",
    "    if transit_chunk_index > 0:\n",
    "        preceding_chunk_start_time = possible_start_times[transit_chunk_index - 1]\n",
    "        logger.info(f\"Found preceding chunk starting at: {preceding_chunk_start_time.iso}\")\n",
    "    else:\n",
    "        logger.error(\"Cannot find a chunk preceding the transit chunk. Need at least two chunks.\")\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Now find the actual complete file sets for these two timestamps using the tolerance\n",
    "    ts1_dt = preceding_chunk_start_time.datetime\n",
    "    ts2_dt = transit_chunk_start_time.datetime\n",
    "    hdf5_sets = {}\n",
    "\n",
    "    all_hdf5 = glob.glob(os.path.join(hdf5_dir, \"20*.hdf5\"))\n",
    "    files_by_approx_ts = defaultdict(list)\n",
    "    for f_path in all_hdf5:\n",
    "         try:\n",
    "             f_name = os.path.basename(f_path)\n",
    "             ts_str = f_name.split('_')[0]\n",
    "             file_dt = datetime.strptime(ts_str, time_format)\n",
    "             # Group files that are close in time to the target start times\n",
    "             if abs((file_dt - ts1_dt).total_seconds()) <= tolerance_sec:\n",
    "                  files_by_approx_ts[ts1_dt.strftime(time_format)].append(f_path)\n",
    "             elif abs((file_dt - ts2_dt).total_seconds()) <= tolerance_sec:\n",
    "                  files_by_approx_ts[ts2_dt.strftime(time_format)].append(f_path)\n",
    "         except Exception:\n",
    "             continue # Ignore files with bad names\n",
    "\n",
    "    # Check completeness for the two target timestamps\n",
    "    expected_subbands = config['services']['hdf5_expected_subbands']\n",
    "    spws_to_include = set(config['ms_creation']['spws'])\n",
    "    ts1_str_exact = ts1_dt.strftime(time_format)\n",
    "    ts2_str_exact = ts2_dt.strftime(time_format)\n",
    "\n",
    "    for ts_key in [ts1_str_exact, ts2_str_exact]:\n",
    "         found_files_for_ts = {}\n",
    "         if ts_key in files_by_approx_ts:\n",
    "              for f_path in files_by_approx_ts[ts_key]:\n",
    "                   try:\n",
    "                        f_name = os.path.basename(f_path)\n",
    "                        spw_str = f_name.split('_')[1].replace('.hdf5', '')\n",
    "                        base_spw = spw_str.split('spl')[0]\n",
    "                        if base_spw in spws_to_include:\n",
    "                             found_files_for_ts[base_spw] = f_path\n",
    "                   except IndexError: continue\n",
    "         if len(found_files_for_ts) == len(spws_to_include):\n",
    "              logger.info(f\"Found complete set for target time {ts_key}\")\n",
    "              sorted_filepaths = [found_files_for_ts[spw] for spw in sorted(list(spws_to_include))]\n",
    "              hdf5_sets[ts_key] = sorted_filepaths\n",
    "         else:\n",
    "              logger.error(f\"Incomplete HDF5 set found for target time {ts_key} ({len(found_files_for_ts)}/{len(spws_to_include)} required SPWs).\")\n",
    "              return None, None, None, None\n",
    "\n",
    "    return hdf5_sets[ts1_str_exact], hdf5_sets[ts2_str_exact], preceding_chunk_start_time, transit_chunk_start_time\n",
    "\n",
    "\n",
    "\n",
    "logging.info(\"Helper functions defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "857806e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-12 08:08:38 [INFO ] [MainThread] [pipeline.config_parser] Loading configuration from: config/pipeline_config.yaml\n",
      "2025-05-12 08:08:38 [DEBUG] [MainThread] [root] Resolved relative path 'ms_stage1_dir': /data/jfaber/dsa110-contimg/pipeline/ms_stage1/\n",
      "2025-05-12 08:08:38 [DEBUG] [MainThread] [root] Resolved relative path 'cal_tables_dir': /data/jfaber/dsa110-contimg/pipeline/cal_tables/\n",
      "2025-05-12 08:08:38 [DEBUG] [MainThread] [root] Resolved relative path 'skymodels_dir': /data/jfaber/dsa110-contimg/pipeline/skymodels/\n",
      "2025-05-12 08:08:38 [DEBUG] [MainThread] [root] Resolved relative path 'images_dir': /data/jfaber/dsa110-contimg/pipeline/images/\n",
      "2025-05-12 08:08:38 [DEBUG] [MainThread] [root] Resolved relative path 'mosaics_dir': /data/jfaber/dsa110-contimg/pipeline/mosaics/\n",
      "2025-05-12 08:08:38 [DEBUG] [MainThread] [root] Resolved relative path 'photometry_dir': /data/jfaber/dsa110-contimg/pipeline/photometry/\n",
      "2025-05-12 08:08:38 [DEBUG] [MainThread] [root] Resolved relative path 'log_dir': /data/jfaber/logs\n",
      "2025-05-12 08:08:38 [DEBUG] [MainThread] [root] Resolved relative path 'diagnostics_base_dir': /data/jfaber/logs/diagnostics\n",
      "2025-05-12 08:08:38 [INFO ] [MainThread] [pipeline.config_parser] Configuration loaded and validated successfully.\n",
      "2025-05-12 08:08:38 [INFO ] [MainThread] [root] Ensuring HDF5 post_handle is set to 'none' for this test run.\n",
      "2025-05-12 08:08:38 [INFO ] [MainThread] [root] --- Stage 0: MANUAL HDF5 Chunk Selection by Nominal Start Time ---\n",
      "2025-05-12 08:08:38 [INFO ] [MainThread] [__main__] Collecting files for nominal start time: 20250507T000500 (parsed as 2025-05-07 00:05:00) in /data/incoming/ with tolerance 120.0s\n",
      "2025-05-12 08:08:38 [DEBUG] [MainThread] [__main__] Expected SPWs: ['sb00', 'sb01', 'sb02', 'sb03', 'sb04', 'sb05', 'sb06', 'sb07', 'sb08', 'sb09', 'sb10', 'sb11', 'sb12', 'sb13', 'sb14', 'sb15']\n",
      "2025-05-12 08:08:39 [DEBUG] [MainThread] [__main__] Found 231496 total HDF5 files in /data/incoming/ to check.\n",
      "2025-05-12 08:08:39 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:04:07_sb09.hdf5: ActualTS=2025-05-07 00:04:07, time_diff=53.0s, parsed_spw='sb09'\n",
      "2025-05-12 08:08:39 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb09'\n",
      "2025-05-12 08:08:39 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:04:06_sb10.hdf5: ActualTS=2025-05-07 00:04:06, time_diff=54.0s, parsed_spw='sb10'\n",
      "2025-05-12 08:08:39 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb10'\n",
      "2025-05-12 08:08:40 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:04:06_sb02.hdf5: ActualTS=2025-05-07 00:04:06, time_diff=54.0s, parsed_spw='sb02'\n",
      "2025-05-12 08:08:40 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb02'\n",
      "2025-05-12 08:08:40 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:04:07_sb04.hdf5: ActualTS=2025-05-07 00:04:07, time_diff=53.0s, parsed_spw='sb04'\n",
      "2025-05-12 08:08:40 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb04'\n",
      "2025-05-12 08:08:40 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:04:07_sb05.hdf5: ActualTS=2025-05-07 00:04:07, time_diff=53.0s, parsed_spw='sb05'\n",
      "2025-05-12 08:08:40 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb05'\n",
      "2025-05-12 08:08:40 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:04:07_sb11.hdf5: ActualTS=2025-05-07 00:04:07, time_diff=53.0s, parsed_spw='sb11'\n",
      "2025-05-12 08:08:40 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb11'\n",
      "2025-05-12 08:08:40 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:04:07_sb15.hdf5: ActualTS=2025-05-07 00:04:07, time_diff=53.0s, parsed_spw='sb15'\n",
      "2025-05-12 08:08:40 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb15'\n",
      "2025-05-12 08:08:40 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:04:07_sb07.hdf5: ActualTS=2025-05-07 00:04:07, time_diff=53.0s, parsed_spw='sb07'\n",
      "2025-05-12 08:08:40 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb07'\n",
      "2025-05-12 08:08:40 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:04:07_sb13.hdf5: ActualTS=2025-05-07 00:04:07, time_diff=53.0s, parsed_spw='sb13'\n",
      "2025-05-12 08:08:40 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb13'\n",
      "2025-05-12 08:08:41 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:04:07_sb06.hdf5: ActualTS=2025-05-07 00:04:07, time_diff=53.0s, parsed_spw='sb06'\n",
      "2025-05-12 08:08:41 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb06'\n",
      "2025-05-12 08:08:41 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:04:07_sb01.hdf5: ActualTS=2025-05-07 00:04:07, time_diff=53.0s, parsed_spw='sb01'\n",
      "2025-05-12 08:08:41 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb01'\n",
      "2025-05-12 08:08:41 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:04:07_sb00.hdf5: ActualTS=2025-05-07 00:04:07, time_diff=53.0s, parsed_spw='sb00'\n",
      "2025-05-12 08:08:41 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb00'\n",
      "2025-05-12 08:08:41 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:04:07_sb12.hdf5: ActualTS=2025-05-07 00:04:07, time_diff=53.0s, parsed_spw='sb12'\n",
      "2025-05-12 08:08:41 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb12'\n",
      "2025-05-12 08:08:41 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:04:07_sb03.hdf5: ActualTS=2025-05-07 00:04:07, time_diff=53.0s, parsed_spw='sb03'\n",
      "2025-05-12 08:08:41 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb03'\n",
      "2025-05-12 08:08:41 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:04:07_sb14.hdf5: ActualTS=2025-05-07 00:04:07, time_diff=53.0s, parsed_spw='sb14'\n",
      "2025-05-12 08:08:41 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb14'\n",
      "2025-05-12 08:08:41 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:04:07_sb08.hdf5: ActualTS=2025-05-07 00:04:07, time_diff=53.0s, parsed_spw='sb08'\n",
      "2025-05-12 08:08:41 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb08'\n",
      "2025-05-12 08:08:41 [INFO ] [MainThread] [__main__] Found complete set of 16 files for nominal start time 20250507T000500\n",
      "2025-05-12 08:08:41 [INFO ] [MainThread] [__main__] Collecting files for nominal start time: 20250507T001000 (parsed as 2025-05-07 00:10:00) in /data/incoming/ with tolerance 120.0s\n",
      "2025-05-12 08:08:41 [DEBUG] [MainThread] [__main__] Expected SPWs: ['sb00', 'sb01', 'sb02', 'sb03', 'sb04', 'sb05', 'sb06', 'sb07', 'sb08', 'sb09', 'sb10', 'sb11', 'sb12', 'sb13', 'sb14', 'sb15']\n",
      "2025-05-12 08:08:42 [DEBUG] [MainThread] [__main__] Found 231496 total HDF5 files in /data/incoming/ to check.\n",
      "2025-05-12 08:08:42 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:09:16_sb05.hdf5: ActualTS=2025-05-07 00:09:16, time_diff=44.0s, parsed_spw='sb05'\n",
      "2025-05-12 08:08:42 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb05'\n",
      "2025-05-12 08:08:42 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:09:16_sb13.hdf5: ActualTS=2025-05-07 00:09:16, time_diff=44.0s, parsed_spw='sb13'\n",
      "2025-05-12 08:08:42 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb13'\n",
      "2025-05-12 08:08:42 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:09:16_sb01.hdf5: ActualTS=2025-05-07 00:09:16, time_diff=44.0s, parsed_spw='sb01'\n",
      "2025-05-12 08:08:42 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb01'\n",
      "2025-05-12 08:08:42 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:09:16_sb10.hdf5: ActualTS=2025-05-07 00:09:16, time_diff=44.0s, parsed_spw='sb10'\n",
      "2025-05-12 08:08:42 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb10'\n",
      "2025-05-12 08:08:42 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:09:16_sb02.hdf5: ActualTS=2025-05-07 00:09:16, time_diff=44.0s, parsed_spw='sb02'\n",
      "2025-05-12 08:08:42 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb02'\n",
      "2025-05-12 08:08:42 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:09:17_sb06.hdf5: ActualTS=2025-05-07 00:09:17, time_diff=43.0s, parsed_spw='sb06'\n",
      "2025-05-12 08:08:42 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb06'\n",
      "2025-05-12 08:08:42 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:09:16_sb07.hdf5: ActualTS=2025-05-07 00:09:16, time_diff=44.0s, parsed_spw='sb07'\n",
      "2025-05-12 08:08:42 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb07'\n",
      "2025-05-12 08:08:42 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:09:16_sb04.hdf5: ActualTS=2025-05-07 00:09:16, time_diff=44.0s, parsed_spw='sb04'\n",
      "2025-05-12 08:08:42 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb04'\n",
      "2025-05-12 08:08:43 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:09:16_sb08.hdf5: ActualTS=2025-05-07 00:09:16, time_diff=44.0s, parsed_spw='sb08'\n",
      "2025-05-12 08:08:43 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb08'\n",
      "2025-05-12 08:08:43 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:09:16_sb00.hdf5: ActualTS=2025-05-07 00:09:16, time_diff=44.0s, parsed_spw='sb00'\n",
      "2025-05-12 08:08:43 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb00'\n",
      "2025-05-12 08:08:43 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:09:16_sb11.hdf5: ActualTS=2025-05-07 00:09:16, time_diff=44.0s, parsed_spw='sb11'\n",
      "2025-05-12 08:08:43 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb11'\n",
      "2025-05-12 08:08:43 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:09:16_sb03.hdf5: ActualTS=2025-05-07 00:09:16, time_diff=44.0s, parsed_spw='sb03'\n",
      "2025-05-12 08:08:43 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb03'\n",
      "2025-05-12 08:08:43 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:09:16_sb15.hdf5: ActualTS=2025-05-07 00:09:16, time_diff=44.0s, parsed_spw='sb15'\n",
      "2025-05-12 08:08:43 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb15'\n",
      "2025-05-12 08:08:43 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:09:16_sb14.hdf5: ActualTS=2025-05-07 00:09:16, time_diff=44.0s, parsed_spw='sb14'\n",
      "2025-05-12 08:08:43 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb14'\n",
      "2025-05-12 08:08:44 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:09:16_sb09.hdf5: ActualTS=2025-05-07 00:09:16, time_diff=44.0s, parsed_spw='sb09'\n",
      "2025-05-12 08:08:44 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb09'\n",
      "2025-05-12 08:08:44 [DEBUG] [MainThread] [__main__]   File 2025-05-07T00:09:16_sb12.hdf5: ActualTS=2025-05-07 00:09:16, time_diff=44.0s, parsed_spw='sb12'\n",
      "2025-05-12 08:08:44 [DEBUG] [MainThread] [__main__]     -> Matched expected SPW: 'sb12'\n",
      "2025-05-12 08:08:44 [INFO ] [MainThread] [__main__] Found complete set of 16 files for nominal start time 20250507T001000\n"
     ]
    }
   ],
   "source": [
    "# Load the main pipeline configuration\n",
    "config = config_parser.load_config(CONFIG_PATH) \n",
    "if not config:\n",
    "    raise ValueError(\"Failed to load configuration.\")\n",
    "\n",
    "config['services']['hdf5_post_handle'] = 'none' \n",
    "logging.info(\"Ensuring HDF5 post_handle is set to 'none' for this test run.\")\n",
    "\n",
    "# --- Stage 0: MANUAL HDF5 Chunk Selection by Nominal Start Time ---\n",
    "logging.info(\"--- Stage 0: MANUAL HDF5 Chunk Selection by Nominal Start Time ---\")\n",
    "\n",
    "HDF5_DIR_MANUAL = config['paths']['hdf5_incoming'] # Or override: '/data/incoming/' \n",
    "\n",
    "# == Specify your desired NOMINAL start times for the two 5-minute chunks ==\n",
    "# For your example (2025-05-07T00:04:06/07), a nominal start might be \"20250507T000400\" or \"20250507T000405\"\n",
    "# The exact nominal value here helps center the search window defined by `same_timestamp_tolerance`.\n",
    "# Let's assume the first 5-min block you want to process starts *nominally* around ts1_manual_nominal_str\n",
    "ts1_manual_nominal_str = \"20250507T000500\"  # first chunk's nominal start time\n",
    "ts2_manual_nominal_str = \"20250507T001000\"  # second chunk's nominal start time\n",
    "# ==========================================================================\n",
    "\n",
    "hdf5_files_1 = collect_files_for_nominal_start_time(ts1_manual_nominal_str, HDF5_DIR_MANUAL, config)\n",
    "hdf5_files_2 = collect_files_for_nominal_start_time(ts2_manual_nominal_str, HDF5_DIR_MANUAL, config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4baf6185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-12 08:08:46 [INFO ] [MainThread] [root] Manually selected HDF5 chunk 1 (Nominal Start: 20250507T000500): Files: ['2025-05-07T00:04:06_sb02.hdf5', '2025-05-07T00:04:06_sb10.hdf5', '2025-05-07T00:04:07_sb00.hdf5', '2025-05-07T00:04:07_sb01.hdf5', '2025-05-07T00:04:07_sb03.hdf5', '2025-05-07T00:04:07_sb04.hdf5', '2025-05-07T00:04:07_sb05.hdf5', '2025-05-07T00:04:07_sb06.hdf5', '2025-05-07T00:04:07_sb07.hdf5', '2025-05-07T00:04:07_sb08.hdf5', '2025-05-07T00:04:07_sb09.hdf5', '2025-05-07T00:04:07_sb11.hdf5', '2025-05-07T00:04:07_sb12.hdf5', '2025-05-07T00:04:07_sb13.hdf5', '2025-05-07T00:04:07_sb14.hdf5', '2025-05-07T00:04:07_sb15.hdf5']\n",
      "2025-05-12 08:08:46 [INFO ] [MainThread] [root] Manually selected HDF5 chunk 2 (Nominal Start: 20250507T001000): Files: ['2025-05-07T00:09:16_sb00.hdf5', '2025-05-07T00:09:16_sb01.hdf5', '2025-05-07T00:09:16_sb02.hdf5', '2025-05-07T00:09:16_sb03.hdf5', '2025-05-07T00:09:16_sb04.hdf5', '2025-05-07T00:09:16_sb05.hdf5', '2025-05-07T00:09:16_sb07.hdf5', '2025-05-07T00:09:16_sb08.hdf5', '2025-05-07T00:09:16_sb09.hdf5', '2025-05-07T00:09:16_sb10.hdf5', '2025-05-07T00:09:16_sb11.hdf5', '2025-05-07T00:09:16_sb12.hdf5', '2025-05-07T00:09:16_sb13.hdf5', '2025-05-07T00:09:16_sb14.hdf5', '2025-05-07T00:09:16_sb15.hdf5', '2025-05-07T00:09:17_sb06.hdf5']\n",
      "2025-05-12 08:08:46 [INFO ] [MainThread] [root] Attempting to determine observation declination from HDF5 metadata...\n",
      "2025-05-12 08:08:47 [DEBUG] [MainThread] [root] Reading metadata from: /data/incoming/2025-03-11T11:05:07_sb00.hdf5\n",
      "2025-05-12 08:08:47 [DEBUG] [MainThread] [h5py._conv] Creating converter from 3 to 5\n",
      "2025-05-12 08:08:47 [INFO ] [MainThread] [root] Determined observation Declination: 16.2734 degrees\n",
      "2025-05-12 08:08:47 [INFO ] [MainThread] [root] Observation Declination set to: 16.2734 degrees for this run.\n",
      "2025-05-12 08:08:47 [INFO ] [MainThread] [__main__] Reading BPCAL candidates from: /data/jfaber/dsa110-contimg/bcal_candidates_vla.csv\n",
      "2025-05-12 08:08:48 [INFO ] [MainThread] [__main__] Selected brightest available BPCAL: 2253+161 (L-Flux: 10.00 Jy)\n",
      "Stored 'ts1_str' (str)\n",
      "Stored 'ts2_str' (str)\n",
      "Stored 'hdf5_files_1' (list)\n",
      "Stored 'hdf5_files_2' (list)\n",
      "Stored 'selected_bcal_info' (dict)\n",
      "Stored 'config' (dict)\n"
     ]
    }
   ],
   "source": [
    "# The 'ts1_str' and 'ts2_str' should be the nominal timestamps used for collection,\n",
    "# as these are used for directory/file naming in subsequent pipeline stages.\n",
    "ts1_str = ts1_manual_nominal_str\n",
    "ts2_str = ts2_manual_nominal_str\n",
    "\n",
    "if not hdf5_files_1 or not hdf5_files_2:\n",
    "    raise RuntimeError(\"Manual HDF5 file selection failed for one or both nominal start times. Check logs and HDF5_DIR.\")\n",
    "\n",
    "logging.info(f\"Manually selected HDF5 chunk 1 (Nominal Start: {ts1_str}): Files: {list(map(os.path.basename, hdf5_files_1))}\")\n",
    "logging.info(f\"Manually selected HDF5 chunk 2 (Nominal Start: {ts2_str}): Files: {list(map(os.path.basename, hdf5_files_2))}\")\n",
    "\n",
    "# --- You still need to select a BPCAL for calibration/imaging metadata ---\n",
    "# Determine observation declination (can still be automatic or you can hardcode it)\n",
    "# It will read one of the files from your HDF5_DIR_MANUAL to get the declination\n",
    "fixed_dec_deg = get_obs_declination(config, HDF5_DIR_MANUAL) \n",
    "if fixed_dec_deg is None: \n",
    "    logging.warning(\"Failed to get observation declination automatically. Using a default or you might need to set it manually.\")\n",
    "    fixed_dec_deg = 67.0 # Example default value, adjust as needed\n",
    "config['calibration']['fixed_declination_deg'] = fixed_dec_deg\n",
    "logging.info(f\"Observation Declination set to: {fixed_dec_deg:.4f} degrees for this run.\")\n",
    "\n",
    "selected_bcal_info = select_bcal_for_test(config, fixed_dec_deg, BCAL_NAME_OVERRIDE) \n",
    "if selected_bcal_info is None: \n",
    "    logging.warning(f\"Failed to select BPCAL for test. Subsequent steps might be affected.\")\n",
    "    # Optionally, provide a default BPCAL dictionary here if needed for the test to proceed\n",
    "    # selected_bcal_info = {'name': '3C286', 'ra': '13h31m08.288s', 'dec': '+30d30m32.96s', \n",
    "    #                       'epoch': 'J2000', 'flux_jy': 14.79, 'ref_freq_ghz': 1.4}\n",
    "\n",
    "\n",
    "# Store variables for the next cells\n",
    "%store ts1_str \n",
    "%store ts2_str \n",
    "%store hdf5_files_1 \n",
    "%store hdf5_files_2 \n",
    "%store selected_bcal_info \n",
    "%store config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcd7c2d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no stored variable or alias #\n",
      "no stored variable or alias Load\n",
      "no stored variable or alias variables\n",
      "2025-05-12 08:08:55 [INFO ] [MainThread] [root] --- Stage 1: MS Creation ---\n",
      "2025-05-12 08:08:55 [INFO ] [MainThread] [pipeline.ms_creation] MS_CREATION - PyUVData version: 3.2.1, Path: /data/jfaber/conda/envs/dsa_contimg/lib/python3.10/site-packages/pyuvdata/__init__.py\n",
      "2025-05-12 08:08:55 [INFO ] [MainThread] [pipeline.ms_creation] Processing HDF5 set for timestamp: 20250507T000500\n",
      "2025-05-12 08:08:55 [INFO ] [MainThread] [pipeline.ms_creation] Loading 16 HDF5 files for one time chunk...\n",
      "2025-05-12 08:08:55 [INFO ] [MainThread] [pipeline.ms_creation] Attempting to read first file: /data/incoming/2025-05-07T00:04:06_sb02.hdf5\n",
      "2025-05-12 08:08:56 [DEBUG] [MainThread] [pipeline.ms_creation] Successfully executed read command for /data/incoming/2025-05-07T00:04:06_sb02.hdf5\n",
      "2025-05-12 08:08:56 [DEBUG] [MainThread] [pipeline.ms_creation] Original uvw_array dtype from /data/incoming/2025-05-07T00:04:06_sb02.hdf5: float32. Converting to float64.\n",
      "2025-05-12 08:08:56 [INFO ] [MainThread] [pipeline.ms_creation] Running pyuvdata check on first file data (/data/incoming/2025-05-07T00:04:06_sb02.hdf5)...\n",
      "2025-05-12 08:08:56 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing UVBase check...\n",
      "2025-05-12 08:08:56 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done UVBase Check\n",
      "2025-05-12 08:08:56 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing Antenna Uniqueness Check...\n",
      "2025-05-12 08:08:56 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Antenna Uniqueness Check\n",
      "2025-05-12 08:08:56 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Setting UVWs from antenna positions...\n",
      "2025-05-12 08:08:56 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Setting UVWs\n",
      "2025-05-12 08:08:56 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Checking autos...\n",
      "2025-05-12 08:08:56 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Checking Autos\n",
      "2025-05-12 08:08:56 [INFO ] [MainThread] [pipeline.ms_creation] pyuvdata check passed for first file data (/data/incoming/2025-05-07T00:04:06_sb02.hdf5).\n",
      "2025-05-12 08:08:56 [DEBUG] [MainThread] [pipeline.ms_creation] Telescope antenna names after first read: ['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '30'\n",
      " '31' '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43' '44'\n",
      " '45' '46' '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57' '58'\n",
      " '59' '60' '61' '62' '63' '64' '65' '66' '67' '68' '69' '70' '71' '72'\n",
      " '73' '74' '75' '76' '77' '78' '79' '80' '81' '82' '83' '84' '85' '86'\n",
      " '87' '88' '89' '90' '91' '92' '93' '94' '95' '96' '97' '98' '99' '100'\n",
      " '101' '102' '103' '104' '105' '106' '107' '108' '109' '110' '111' '112'\n",
      " '113' '114' '115' '116' '117']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The uvw_array does not match the expected values given the antenna positions. The largest discrepancy is 1253.2069264236213 meters. This is a fairly common situation but might indicate an error in the antenna positions, the uvws or the phasing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-12 08:08:57 [DEBUG] [MainThread] [pipeline.ms_creation] Reading subsequent file 1: /data/incoming/2025-05-07T00:04:06_sb10.hdf5\n",
      "2025-05-12 08:08:57 [DEBUG] [MainThread] [pipeline.ms_creation] Converting uvw_array for /data/incoming/2025-05-07T00:04:06_sb10.hdf5 to float64.\n",
      "2025-05-12 08:08:57 [DEBUG] [MainThread] [pipeline.ms_creation] Successfully read and processed subsequent file /data/incoming/2025-05-07T00:04:06_sb10.hdf5\n",
      "2025-05-12 08:08:57 [DEBUG] [MainThread] [pipeline.ms_creation] Reading subsequent file 2: /data/incoming/2025-05-07T00:04:07_sb00.hdf5\n",
      "2025-05-12 08:08:58 [DEBUG] [MainThread] [pipeline.ms_creation] Converting uvw_array for /data/incoming/2025-05-07T00:04:07_sb00.hdf5 to float64.\n",
      "2025-05-12 08:08:58 [DEBUG] [MainThread] [pipeline.ms_creation] Successfully read and processed subsequent file /data/incoming/2025-05-07T00:04:07_sb00.hdf5\n",
      "2025-05-12 08:08:58 [DEBUG] [MainThread] [pipeline.ms_creation] Reading subsequent file 3: /data/incoming/2025-05-07T00:04:07_sb01.hdf5\n",
      "2025-05-12 08:08:59 [DEBUG] [MainThread] [pipeline.ms_creation] Converting uvw_array for /data/incoming/2025-05-07T00:04:07_sb01.hdf5 to float64.\n",
      "2025-05-12 08:08:59 [DEBUG] [MainThread] [pipeline.ms_creation] Successfully read and processed subsequent file /data/incoming/2025-05-07T00:04:07_sb01.hdf5\n",
      "2025-05-12 08:08:59 [DEBUG] [MainThread] [pipeline.ms_creation] Reading subsequent file 4: /data/incoming/2025-05-07T00:04:07_sb03.hdf5\n",
      "2025-05-12 08:09:00 [DEBUG] [MainThread] [pipeline.ms_creation] Converting uvw_array for /data/incoming/2025-05-07T00:04:07_sb03.hdf5 to float64.\n",
      "2025-05-12 08:09:00 [DEBUG] [MainThread] [pipeline.ms_creation] Successfully read and processed subsequent file /data/incoming/2025-05-07T00:04:07_sb03.hdf5\n",
      "2025-05-12 08:09:00 [DEBUG] [MainThread] [pipeline.ms_creation] Reading subsequent file 5: /data/incoming/2025-05-07T00:04:07_sb04.hdf5\n",
      "2025-05-12 08:09:00 [DEBUG] [MainThread] [pipeline.ms_creation] Converting uvw_array for /data/incoming/2025-05-07T00:04:07_sb04.hdf5 to float64.\n",
      "2025-05-12 08:09:00 [DEBUG] [MainThread] [pipeline.ms_creation] Successfully read and processed subsequent file /data/incoming/2025-05-07T00:04:07_sb04.hdf5\n",
      "2025-05-12 08:09:01 [DEBUG] [MainThread] [pipeline.ms_creation] Reading subsequent file 6: /data/incoming/2025-05-07T00:04:07_sb05.hdf5\n",
      "2025-05-12 08:09:01 [DEBUG] [MainThread] [pipeline.ms_creation] Converting uvw_array for /data/incoming/2025-05-07T00:04:07_sb05.hdf5 to float64.\n",
      "2025-05-12 08:09:01 [DEBUG] [MainThread] [pipeline.ms_creation] Successfully read and processed subsequent file /data/incoming/2025-05-07T00:04:07_sb05.hdf5\n",
      "2025-05-12 08:09:02 [DEBUG] [MainThread] [pipeline.ms_creation] Reading subsequent file 7: /data/incoming/2025-05-07T00:04:07_sb06.hdf5\n",
      "2025-05-12 08:09:02 [DEBUG] [MainThread] [pipeline.ms_creation] Converting uvw_array for /data/incoming/2025-05-07T00:04:07_sb06.hdf5 to float64.\n",
      "2025-05-12 08:09:02 [DEBUG] [MainThread] [pipeline.ms_creation] Successfully read and processed subsequent file /data/incoming/2025-05-07T00:04:07_sb06.hdf5\n",
      "2025-05-12 08:09:02 [DEBUG] [MainThread] [pipeline.ms_creation] Reading subsequent file 8: /data/incoming/2025-05-07T00:04:07_sb07.hdf5\n",
      "2025-05-12 08:09:03 [DEBUG] [MainThread] [pipeline.ms_creation] Converting uvw_array for /data/incoming/2025-05-07T00:04:07_sb07.hdf5 to float64.\n",
      "2025-05-12 08:09:03 [DEBUG] [MainThread] [pipeline.ms_creation] Successfully read and processed subsequent file /data/incoming/2025-05-07T00:04:07_sb07.hdf5\n",
      "2025-05-12 08:09:03 [DEBUG] [MainThread] [pipeline.ms_creation] Reading subsequent file 9: /data/incoming/2025-05-07T00:04:07_sb08.hdf5\n",
      "2025-05-12 08:09:04 [DEBUG] [MainThread] [pipeline.ms_creation] Converting uvw_array for /data/incoming/2025-05-07T00:04:07_sb08.hdf5 to float64.\n",
      "2025-05-12 08:09:04 [DEBUG] [MainThread] [pipeline.ms_creation] Successfully read and processed subsequent file /data/incoming/2025-05-07T00:04:07_sb08.hdf5\n",
      "2025-05-12 08:09:04 [DEBUG] [MainThread] [pipeline.ms_creation] Reading subsequent file 10: /data/incoming/2025-05-07T00:04:07_sb09.hdf5\n",
      "2025-05-12 08:09:05 [DEBUG] [MainThread] [pipeline.ms_creation] Converting uvw_array for /data/incoming/2025-05-07T00:04:07_sb09.hdf5 to float64.\n",
      "2025-05-12 08:09:05 [DEBUG] [MainThread] [pipeline.ms_creation] Successfully read and processed subsequent file /data/incoming/2025-05-07T00:04:07_sb09.hdf5\n",
      "2025-05-12 08:09:05 [DEBUG] [MainThread] [pipeline.ms_creation] Reading subsequent file 11: /data/incoming/2025-05-07T00:04:07_sb11.hdf5\n",
      "2025-05-12 08:09:05 [DEBUG] [MainThread] [pipeline.ms_creation] Converting uvw_array for /data/incoming/2025-05-07T00:04:07_sb11.hdf5 to float64.\n",
      "2025-05-12 08:09:05 [DEBUG] [MainThread] [pipeline.ms_creation] Successfully read and processed subsequent file /data/incoming/2025-05-07T00:04:07_sb11.hdf5\n",
      "2025-05-12 08:09:06 [DEBUG] [MainThread] [pipeline.ms_creation] Reading subsequent file 12: /data/incoming/2025-05-07T00:04:07_sb12.hdf5\n",
      "2025-05-12 08:09:06 [DEBUG] [MainThread] [pipeline.ms_creation] Converting uvw_array for /data/incoming/2025-05-07T00:04:07_sb12.hdf5 to float64.\n",
      "2025-05-12 08:09:06 [DEBUG] [MainThread] [pipeline.ms_creation] Successfully read and processed subsequent file /data/incoming/2025-05-07T00:04:07_sb12.hdf5\n",
      "2025-05-12 08:09:07 [DEBUG] [MainThread] [pipeline.ms_creation] Reading subsequent file 13: /data/incoming/2025-05-07T00:04:07_sb13.hdf5\n",
      "2025-05-12 08:09:07 [DEBUG] [MainThread] [pipeline.ms_creation] Converting uvw_array for /data/incoming/2025-05-07T00:04:07_sb13.hdf5 to float64.\n",
      "2025-05-12 08:09:07 [DEBUG] [MainThread] [pipeline.ms_creation] Successfully read and processed subsequent file /data/incoming/2025-05-07T00:04:07_sb13.hdf5\n",
      "2025-05-12 08:09:07 [DEBUG] [MainThread] [pipeline.ms_creation] Reading subsequent file 14: /data/incoming/2025-05-07T00:04:07_sb14.hdf5\n",
      "2025-05-12 08:09:08 [DEBUG] [MainThread] [pipeline.ms_creation] Converting uvw_array for /data/incoming/2025-05-07T00:04:07_sb14.hdf5 to float64.\n",
      "2025-05-12 08:09:08 [DEBUG] [MainThread] [pipeline.ms_creation] Successfully read and processed subsequent file /data/incoming/2025-05-07T00:04:07_sb14.hdf5\n",
      "2025-05-12 08:09:08 [DEBUG] [MainThread] [pipeline.ms_creation] Reading subsequent file 15: /data/incoming/2025-05-07T00:04:07_sb15.hdf5\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pipeline.ms_creation] Converting uvw_array for /data/incoming/2025-05-07T00:04:07_sb15.hdf5 to float64.\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pipeline.ms_creation] Successfully read and processed subsequent file /data/incoming/2025-05-07T00:04:07_sb15.hdf5\n",
      "2025-05-12 08:09:09 [INFO ] [MainThread] [pipeline.ms_creation] Concatenating 15 additional frequency chunks.\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing UVBase check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done UVBase Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing Antenna Uniqueness Check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Antenna Uniqueness Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing UVBase check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done UVBase Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing Antenna Uniqueness Check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Antenna Uniqueness Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing UVBase check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done UVBase Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing Antenna Uniqueness Check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Antenna Uniqueness Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing UVBase check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done UVBase Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing Antenna Uniqueness Check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Antenna Uniqueness Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing UVBase check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done UVBase Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing Antenna Uniqueness Check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Antenna Uniqueness Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing UVBase check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done UVBase Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing Antenna Uniqueness Check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Antenna Uniqueness Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing UVBase check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done UVBase Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing Antenna Uniqueness Check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Antenna Uniqueness Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing UVBase check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done UVBase Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing Antenna Uniqueness Check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Antenna Uniqueness Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing UVBase check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done UVBase Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing Antenna Uniqueness Check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Antenna Uniqueness Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing UVBase check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done UVBase Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing Antenna Uniqueness Check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Antenna Uniqueness Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing UVBase check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done UVBase Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing Antenna Uniqueness Check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Antenna Uniqueness Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing UVBase check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done UVBase Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing Antenna Uniqueness Check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Antenna Uniqueness Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing UVBase check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done UVBase Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing Antenna Uniqueness Check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Antenna Uniqueness Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing UVBase check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done UVBase Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing Antenna Uniqueness Check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Antenna Uniqueness Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing UVBase check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done UVBase Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing Antenna Uniqueness Check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Antenna Uniqueness Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing UVBase check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done UVBase Check\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing Antenna Uniqueness Check...\n",
      "2025-05-12 08:09:09 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Antenna Uniqueness Check\n",
      "2025-05-12 08:09:17 [INFO ] [MainThread] [pipeline.ms_creation] Concatenation complete. Running pyuvdata check...\n",
      "2025-05-12 08:09:17 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing UVBase check...\n",
      "2025-05-12 08:09:17 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done UVBase Check\n",
      "2025-05-12 08:09:17 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Doing Antenna Uniqueness Check...\n",
      "2025-05-12 08:09:17 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Antenna Uniqueness Check\n",
      "2025-05-12 08:09:17 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Setting UVWs from antenna positions...\n",
      "2025-05-12 08:09:17 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Setting UVWs\n",
      "2025-05-12 08:09:17 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] Checking autos...\n",
      "2025-05-12 08:09:17 [DEBUG] [MainThread] [pyuvdata.uvdata.uvdata] ... Done Checking Autos\n",
      "2025-05-12 08:09:17 [INFO ] [MainThread] [pipeline.ms_creation] pyuvdata check passed after concatenation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The uvw_array does not match the expected values given the antenna positions. The largest discrepancy is 1253.2069264236213 meters. This is a fairly common situation but might indicate an error in the antenna positions, the uvws or the phasing.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Final telescope antenna names in _load_uvh5_file: ['1' '2' '3' '4' '5' '6' '7' '8' '9' '10' '11' '12' '13' '14' '15' '16'\n",
      " '17' '18' '19' '20' '21' '22' '23' '24' '25' '26' '27' '28' '29' '30'\n",
      " '31' '32' '33' '34' '35' '36' '37' '38' '39' '40' '41' '42' '43' '44'\n",
      " '45' '46' '47' '48' '49' '50' '51' '52' '53' '54' '55' '56' '57' '58'\n",
      " '59' '60' '61' '62' '63' '64' '65' '66' '67' '68' '69' '70' '71' '72'\n",
      " '73' '74' '75' '76' '77' '78' '79' '80' '81' '82' '83' '84' '85' '86'\n",
      " '87' '88' '89' '90' '91' '92' '93' '94' '95' '96' '97' '98' '99' '100'\n",
      " '101' '102' '103' '104' '105' '106' '107' '108' '109' '110' '111' '112'\n",
      " '113' '114' '115' '116' '117']\n",
      "2025-05-12 08:09:31 [INFO ] [MainThread] [pipeline.ms_creation] Finished loading data. Nbls: 4656, Ntimes: 24, Nfreqs: 768, Nants: 96\n",
      "2025-05-12 08:09:31 [INFO ] [MainThread] [pipeline.ms_creation] Calculating and setting phase centers for drift scan.\n",
      "2025-05-12 08:09:31 [INFO ] [MainThread] [pipeline.ms_creation] _set_phase_centers - PyUVData version: 3.2.1, Path: /data/jfaber/conda/envs/dsa_contimg/lib/python3.10/site-packages/pyuvdata/__init__.py\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 0: drift_T0000_RA107.844 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 1: drift_T0001_RA107.897 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 2: drift_T0002_RA107.951 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 3: drift_T0003_RA108.005 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 4: drift_T0004_RA108.059 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 5: drift_T0005_RA108.113 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 6: drift_T0006_RA108.167 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 7: drift_T0007_RA108.220 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 8: drift_T0008_RA108.274 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 9: drift_T0009_RA108.328 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 10: drift_T0010_RA108.382 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 11: drift_T0011_RA108.436 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 12: drift_T0012_RA108.490 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 13: drift_T0013_RA108.543 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 14: drift_T0014_RA108.597 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 15: drift_T0015_RA108.651 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 16: drift_T0016_RA108.705 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 17: drift_T0017_RA108.759 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 18: drift_T0018_RA108.813 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 19: drift_T0019_RA108.866 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 20: drift_T0020_RA108.920 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 21: drift_T0021_RA108.974 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 22: drift_T0022_RA109.028 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Added phase center ID 23: drift_T0023_RA109.082 with type 'sidereal'\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] phase_center_id_array set with 111744 elements.\n",
      "2025-05-12 08:09:31 [INFO ] [MainThread] [pipeline.ms_creation] Manually looping through phase centers to apply phasing and update UVWs.\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] LST array already exists.\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 0 (drift_T0000_RA107.844) using boolean_select_mask.\n",
      "2025-05-12 08:09:31 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 1 (drift_T0001_RA107.897) using boolean_select_mask.\n",
      "2025-05-12 08:09:32 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 2 (drift_T0002_RA107.951) using boolean_select_mask.\n",
      "2025-05-12 08:09:33 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 3 (drift_T0003_RA108.005) using boolean_select_mask.\n",
      "2025-05-12 08:09:33 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 4 (drift_T0004_RA108.059) using boolean_select_mask.\n",
      "2025-05-12 08:09:34 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 5 (drift_T0005_RA108.113) using boolean_select_mask.\n",
      "2025-05-12 08:09:34 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 6 (drift_T0006_RA108.167) using boolean_select_mask.\n",
      "2025-05-12 08:09:35 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 7 (drift_T0007_RA108.220) using boolean_select_mask.\n",
      "2025-05-12 08:09:36 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 8 (drift_T0008_RA108.274) using boolean_select_mask.\n",
      "2025-05-12 08:09:36 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 9 (drift_T0009_RA108.328) using boolean_select_mask.\n",
      "2025-05-12 08:09:37 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 10 (drift_T0010_RA108.382) using boolean_select_mask.\n",
      "2025-05-12 08:09:38 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 11 (drift_T0011_RA108.436) using boolean_select_mask.\n",
      "2025-05-12 08:09:38 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 12 (drift_T0012_RA108.490) using boolean_select_mask.\n",
      "2025-05-12 08:09:39 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 13 (drift_T0013_RA108.543) using boolean_select_mask.\n",
      "2025-05-12 08:09:39 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 14 (drift_T0014_RA108.597) using boolean_select_mask.\n",
      "2025-05-12 08:09:40 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 15 (drift_T0015_RA108.651) using boolean_select_mask.\n",
      "2025-05-12 08:09:41 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 16 (drift_T0016_RA108.705) using boolean_select_mask.\n",
      "2025-05-12 08:09:41 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 17 (drift_T0017_RA108.759) using boolean_select_mask.\n",
      "2025-05-12 08:09:42 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 18 (drift_T0018_RA108.813) using boolean_select_mask.\n",
      "2025-05-12 08:09:42 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 19 (drift_T0019_RA108.866) using boolean_select_mask.\n",
      "2025-05-12 08:09:43 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 20 (drift_T0020_RA108.920) using boolean_select_mask.\n",
      "2025-05-12 08:09:44 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 21 (drift_T0021_RA108.974) using boolean_select_mask.\n",
      "2025-05-12 08:09:44 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 22 (drift_T0022_RA109.028) using boolean_select_mask.\n",
      "2025-05-12 08:09:45 [DEBUG] [MainThread] [pipeline.ms_creation] Phasing data subset for ID 23 (drift_T0023_RA109.082) using boolean_select_mask.\n",
      "2025-05-12 08:09:45 [INFO ] [MainThread] [pipeline.ms_creation] Manual phasing loop completed for 24 unique phase centers.\n",
      "2025-05-12 08:09:45 [INFO ] [MainThread] [pipeline.ms_creation] No calibrator model requested in config.\n",
      "2025-05-12 08:09:45 [INFO ] [MainThread] [pipeline.ms_creation] Writing Measurement Set to: /data/jfaber/dsa110-contimg/pipeline/ms_stage1/drift_20250507T000500.ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UVData object contains a mix of baseline conjugation states, which is not uniformly supported in CASA -- forcing conjugation to be \"ant2<ant1\" on object.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Notebook Cell: MS Creation\n",
    "%store -r ts1_str ts2_str hdf5_files_1 hdf5_files_2 selected_bcal_info config # Load variables\n",
    "\n",
    "logging.info(\"--- Stage 1: MS Creation ---\")\n",
    "ms_path_1 = ms_creation.process_hdf5_set(config, ts1_str, hdf5_files_1)\n",
    "ms_path_2 = ms_creation.process_hdf5_set(config, ts2_str, hdf5_files_2)\n",
    "\n",
    "if not ms_path_1 or not ms_path_2:\n",
    "    raise RuntimeError(\"MS Creation failed for one or both chunks.\")\n",
    "\n",
    "logging.info(f\"Created MS files: {os.path.basename(ms_path_1)}, {os.path.basename(ms_path_2)}\")\n",
    "ms_files_to_process = [ms_path_1, ms_path_2]\n",
    "\n",
    "# Store paths for next cell\n",
    "%store ms_files_to_process selected_bcal_info config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "966ea6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Cell: Calibration and Imaging\n",
    "%store -r ms_files_to_process selected_bcal_info config # Load variables\n",
    "\n",
    "logging.info(\"--- Stage 2: Calibration and Imaging ---\")\n",
    "processed_images = []\n",
    "processed_pbs = []\n",
    "block_mask_path = None\n",
    "template_image_path = None\n",
    "gcal_table_path = None\n",
    "cl_path_bcal = None\n",
    "paths_config = config['paths'] # Get paths config\n",
    "\n",
    "# 2a. Find latest BPCAL table\n",
    "try:\n",
    "    cal_tables_dir = paths_config['cal_tables_dir']\n",
    "    bcal_files = sorted(glob.glob(os.path.join(cal_tables_dir, \"*.bcal\")))\n",
    "    if not bcal_files: raise RuntimeError(f\"No BPCAL tables (*.bcal) found in {cal_tables_dir}.\")\n",
    "    latest_bcal_table = bcal_files[-1]\n",
    "    logging.info(f\"Using BPCAL table: {os.path.basename(latest_bcal_table)}\")\n",
    "except Exception as e:\n",
    "    logging.critical(f\"Failed to find BPCAL table: {e}. Aborting test.\")\n",
    "    raise e # Stop execution\n",
    "\n",
    "# 2b. Generate Calibrator Model & Gain Cal Table (using transit chunk only)\n",
    "try:\n",
    "    skymodels_dir = paths_config['skymodels_dir']\n",
    "    cl_bcal_filename = f\"bcal_sky_{selected_bcal_info['name']}_test.cl\" # Add suffix\n",
    "    cl_bcal_output_path = os.path.join(skymodels_dir, cl_bcal_filename)\n",
    "    cl_path_bcal, _ = skymodel.create_calibrator_component_list(config, selected_bcal_info, cl_bcal_output_path)\n",
    "    if not cl_path_bcal: raise RuntimeError(\"Failed to create BPCAL sky model.\")\n",
    "\n",
    "    # Use the second MS (transit chunk) for gain cal\n",
    "    ms_path_transit = ms_files_to_process[1]\n",
    "    ts_transit = os.path.basename(ms_path_transit).split('_')[1].replace('.ms', '')\n",
    "    logging.info(f\"Performing gain calibration on transit chunk: {os.path.basename(ms_path_transit)}\")\n",
    "    gcal_time_str = f\"bcal_test_{ts_transit}\"\n",
    "    gcal_table_path = calibration.perform_gain_calibration(config, [ms_path_transit], cl_path_bcal, gcal_time_str, solint='inf')\n",
    "    if not gcal_table_path: raise RuntimeError(\"Gain calibration on BPCAL failed.\")\n",
    "    logging.info(f\"Gain table generated: {os.path.basename(gcal_table_path)}\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Failed during gain calibration setup stage: {e}\", exc_info=True)\n",
    "    logging.warning(\"Proceeding without gain calibration solutions.\")\n",
    "    gcal_table_path = []\n",
    "\n",
    "# 2c. Prepare Mask (using BPCAL model, defer creation until template exists)\n",
    "use_mask_config = config.get('imaging',{}).get('use_clean_mask', False)\n",
    "mask_output_path = None\n",
    "if use_mask_config and cl_path_bcal:\n",
    "    mask_output_path = os.path.join(skymodels_dir, f\"mask_bcal_test_{selected_bcal_info['name']}.mask\")\n",
    "    logging.info(f\"Will attempt to create mask: {mask_output_path}\")\n",
    "else:\n",
    "    logging.info(\"Masking disabled or BPCAL model missing, skipping mask.\")\n",
    "mask_created = False\n",
    "\n",
    "# 2d. Loop through MS files\n",
    "images_dir = paths_config['images_dir']\n",
    "for i, ms_path in enumerate(ms_files_to_process):\n",
    "    logging.info(f\"Processing MS {i+1}/{len(ms_files_to_process)}: {os.path.basename(ms_path)}\")\n",
    "    ms_base = os.path.splitext(os.path.basename(ms_path))[0]\n",
    "    image_base = os.path.join(images_dir, f\"{ms_base}_test\")\n",
    "\n",
    "    try:\n",
    "        if not calibration.flag_rfi(config, ms_path): raise RuntimeError(\"RFI Flagging failed.\")\n",
    "        if not calibration.flag_general(config, ms_path): raise RuntimeError(\"General Flagging failed.\")\n",
    "\n",
    "        gcal_list = [gcal_table_path] if gcal_table_path and isinstance(gcal_table_path, str) else []\n",
    "        if not calibration.apply_calibration(config, ms_path, latest_bcal_table, gcal_list):\n",
    "            raise RuntimeError(\"ApplyCal failed.\")\n",
    "\n",
    "        ms_to_image = ms_path\n",
    "        current_mask_path = None\n",
    "        if use_mask_config and mask_output_path:\n",
    "            if not mask_created:\n",
    "                if template_image_path:\n",
    "                    logging.info(f\"Creating block mask {mask_output_path} using template {template_image_path}\")\n",
    "                    if imaging.create_clean_mask(config, cl_path_bcal, template_image_path, mask_output_path):\n",
    "                        mask_created = True\n",
    "                    else: logging.warning(\"Failed to create mask. Proceeding without.\")\n",
    "                else: logging.debug(\"Template image not yet available for mask creation.\")\n",
    "            if mask_created: current_mask_path = mask_output_path\n",
    "\n",
    "        logging.info(\"Running tclean...\")\n",
    "        tclean_image_basename = imaging.run_tclean(config, ms_to_image, image_base, cl_path=None, mask_path=current_mask_path)\n",
    "\n",
    "        if tclean_image_basename:\n",
    "            img_path = f\"{tclean_image_basename}.image\"\n",
    "            pb_path = f\"{tclean_image_basename}.pb\"\n",
    "            if os.path.exists(img_path) and os.path.exists(pb_path):\n",
    "                processed_images.append(img_path); processed_pbs.append(pb_path)\n",
    "                logging.info(f\"Successfully imaged {ms_path}\")\n",
    "                if template_image_path is None: template_image_path = img_path\n",
    "            else: raise RuntimeError(f\"tclean image/pb missing for {tclean_image_basename}\")\n",
    "        else: raise RuntimeError(\"tclean failed.\")\n",
    "\n",
    "    except Exception as e_ms:\n",
    "        logging.error(f\"Failed processing MS {ms_path}: {e_ms}\", exc_info=True)\n",
    "        raise e_ms # Stop execution on failure\n",
    "\n",
    "# Store results for next cell\n",
    "%store processed_images processed_pbs config selected_bcal_info ts1_str ts2_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506999eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Cell: Mosaicking\n",
    "%store -r processed_images processed_pbs config selected_bcal_info ts1_str ts2_str # Load variables\n",
    "\n",
    "mosaic_img_path = None\n",
    "if len(processed_images) == 2:\n",
    "    logging.info(\"--- Stage 3: Mosaicking ---\")\n",
    "    # Use timestamps from original chunks for naming\n",
    "    mosaic_basename = f\"mosaic_test_{ts1_str}_{ts2_str}\"\n",
    "    try:\n",
    "        mosaic_img_path, _ = mosaicking.create_mosaic(config, processed_images, processed_pbs, mosaic_basename)\n",
    "        if not mosaic_img_path: raise RuntimeError(\"Mosaicking function returned None.\")\n",
    "        logging.info(f\"Mosaic created: {mosaic_img_path}\")\n",
    "        # Store for next cell\n",
    "        %store mosaic_img_path config selected_bcal_info ts1_str ts2_str\n",
    "    except Exception as e_mosaic:\n",
    "        logging.error(f\"Mosaicking failed: {e_mosaic}\", exc_info=True)\n",
    "        raise e_mosaic # Stop execution\n",
    "else:\n",
    "    raise RuntimeError(f\"Could not proceed to mosaicking: Only {len(processed_images)} images were created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb30f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook Cell: Photometry\n",
    "%store -r mosaic_img_path config selected_bcal_info ts1_str ts2_str # Load variables\n",
    "\n",
    "if mosaic_img_path:\n",
    "    logging.info(\"--- Stage 4: Photometry ---\")\n",
    "    mosaic_fits_path = f\"{os.path.splitext(mosaic_img_path)[0]}.linmos.fits\"\n",
    "    if not os.path.exists(mosaic_fits_path):\n",
    "         logging.warning(f\"Mosaic FITS {mosaic_fits_path} not found, attempting export...\")\n",
    "         mosaic_fits_path = imaging.export_image_to_fits(config, mosaic_img_path, suffix='.linmos')\n",
    "\n",
    "    if mosaic_fits_path and os.path.exists(mosaic_fits_path):\n",
    "        logging.info(f\"Running photometry on mosaic: {mosaic_fits_path}\")\n",
    "        try:\n",
    "            targets, references = photometry.identify_sources(config, mosaic_fits_path)\n",
    "            # Convert to pandas DataFrames for easier handling below\n",
    "            phot_targets_df = pd.DataFrame(targets) if targets is not None else pd.DataFrame()\n",
    "            phot_references_df = pd.DataFrame(references) if references is not None else pd.DataFrame()\n",
    "\n",
    "            # Add BPCAL to targets list if not already there\n",
    "            if selected_bcal_info and selected_bcal_info['name'] not in phot_targets_df['name'].values:\n",
    "                 try:\n",
    "                      bcal_coord = SkyCoord(ra=selected_bcal_info['ra'], dec=selected_bcal_info['dec'], unit=(u.hourangle, u.deg), frame='icrs')\n",
    "                      with fits.open(mosaic_fits_path) as hdul: wcs = WCS(hdul[0].header).celestial\n",
    "                      xpix, ypix = wcs.world_to_pixel(bcal_coord)\n",
    "                      # Create row ensuring necessary columns exist\n",
    "                      bcal_row_data = {'name': selected_bcal_info['name'], 'source_id': selected_bcal_info['name'],\n",
    "                                      'RAJ2000': selected_bcal_info['ra'], 'DEC_J2000': selected_bcal_info['dec'],\n",
    "                                      'xpix': xpix, 'ypix': ypix}\n",
    "                      for col in phot_targets_df.columns:\n",
    "                           if col not in bcal_row_data: bcal_row_data[col] = np.nan\n",
    "                      phot_targets_df = pd.concat([phot_targets_df, pd.DataFrame([bcal_row_data])], ignore_index=True)\n",
    "                      logging.info(f\"Added BPCAL {selected_bcal_info['name']} to target list for photometry.\")\n",
    "                 except Exception as e_add: logging.warning(f\"Could not add BPCAL to target list: {e_add}\")\n",
    "\n",
    "\n",
    "            if not phot_targets_df.empty and not phot_references_df.empty:\n",
    "                phot_table = photometry.perform_aperture_photometry(config, mosaic_fits_path, phot_targets_df, phot_references_df)\n",
    "                if phot_table is not None:\n",
    "                    rel_flux_table = photometry.calculate_relative_fluxes(config, phot_table) # Assumes returns DF\n",
    "                    if rel_flux_table is not None:\n",
    "                        logging.info(\"Photometry successful. Relative flux results:\")\n",
    "                        print(\"\\n--- Relative Photometry Results ---\")\n",
    "                        # Display relevant columns using pandas display\n",
    "                        display_cols = ['source_id', 'relative_flux', 'relative_flux_error', 'median_reference_flux', 'reference_source_ids']\n",
    "                        # Ensure columns exist before displaying\n",
    "                        display_cols = [col for col in display_cols if col in rel_flux_table.columns]\n",
    "                        display(rel_flux_table[display_cols]) # Use IPython display\n",
    "\n",
    "                        # Save to a test CSV\n",
    "                        test_output_csv = os.path.join(config['paths']['photometry_dir'], f\"test_photometry_{ts1_str}_{ts2_str}.csv\")\n",
    "                        rel_flux_table.to_csv(test_output_csv, index=False, float_format='%.4f', na_rep='NaN')\n",
    "                        logging.info(f\"Saved test photometry results to: {test_output_csv}\")\n",
    "                    else: logging.error(\"Relative flux calculation failed.\")\n",
    "                else: logging.error(\"Aperture photometry failed.\")\n",
    "            elif phot_targets_df.empty: logging.warning(\"No target sources identified/valid for photometry.\")\n",
    "            else: logging.error(\"Reference source identification failed or references missing.\")\n",
    "        except Exception as e_phot: logging.error(f\"Photometry stage failed: {e_phot}\", exc_info=True)\n",
    "    else: logging.error(f\"Mosaic FITS file missing: {mosaic_fits_path}. Cannot run photometry.\")\n",
    "\n",
    "logging.info(\"--- Notebook Test Run Finished ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3726ff4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsa_contimg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
