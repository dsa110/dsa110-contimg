#!/opt/miniforge/envs/casa6/bin/python
"""
Performance benchmarking suite for Absurd workflow manager.

Measures throughput, latency, and resource usage under various workloads.
"""

from __future__ import annotations

import argparse
import asyncio
import json
import os
import statistics
import time
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any, Dict, List
from uuid import uuid4

import psutil
from tabulate import tabulate

from dsa110_contimg.absurd import AbsurdClient, AbsurdConfig


@dataclass
class BenchmarkResult:
    """Results from a benchmark run."""

    name: str
    num_tasks: int
    duration_sec: float
    throughput: float  # tasks/sec
    latency_p50: float  # seconds
    latency_p95: float  # seconds
    latency_p99: float  # seconds
    memory_used_mb: float
    cpu_percent: float
    errors: int
    success_rate: float


class AbsurdBenchmark:
    """Absurd performance benchmark suite."""

    def __init__(self, database_url: str, queue_name: str):
        self.database_url = database_url
        self.queue_name = queue_name
        self.client: Optional[AbsurdClient] = None
        self.results: List[BenchmarkResult] = []

    async def setup(self):
        """Initialize benchmark environment."""
        self.client = AbsurdClient(self.database_url, pool_min_size=5, pool_max_size=20)
        await self.client.connect()

        # Clean queue
        tasks = await self.client.list_tasks(queue_name=self.queue_name, limit=10000)
        for task in tasks:
            try:
                await self.client.cancel_task(task["task_id"])
            except Exception:
                pass

    async def teardown(self):
        """Clean up benchmark environment."""
        if self.client:
            await self.client.close()

    async def benchmark_spawn_throughput(self, num_tasks: int = 1000):
        """Benchmark task spawning throughput."""
        print(f"\n:rocket: Benchmarking task spawning ({num_tasks} tasks)...")

        process = psutil.Process()
        start_mem = process.memory_info().rss / 1024 / 1024
        start_cpu = process.cpu_percent()

        start = time.time()

        task_ids = await asyncio.gather(
            *[
                self.client.spawn_task(
                    queue_name=self.queue_name,
                    task_name=f"bench-task-{i}",
                    params={"index": i},
                    priority=5,
                )
                for i in range(num_tasks)
            ],
            return_exceptions=True,
        )

        duration = time.time() - start

        end_mem = process.memory_info().rss / 1024 / 1024
        end_cpu = process.cpu_percent()

        errors = sum(1 for t in task_ids if isinstance(t, Exception))
        success_count = num_tasks - errors

        result = BenchmarkResult(
            name="spawn_throughput",
            num_tasks=num_tasks,
            duration_sec=duration,
            throughput=success_count / duration,
            latency_p50=0,
            latency_p95=0,
            latency_p99=0,
            memory_used_mb=end_mem - start_mem,
            cpu_percent=(end_cpu - start_cpu),
            errors=errors,
            success_rate=success_count / num_tasks,
        )

        self.results.append(result)

        print(f"  :check_mark: Spawned {success_count} tasks in {duration:.2f}s")
        print(f"  :check_mark: Throughput: {result.throughput:.1f} tasks/s")
        print(f"  :check_mark: Memory: +{result.memory_used_mb:.1f} MB")

        return result

    async def benchmark_claim_throughput(self, num_tasks: int = 1000):
        """Benchmark task claiming throughput."""
        print(f"\n:direct_hit: Benchmarking task claiming ({num_tasks} tasks)...")

        # First spawn tasks
        await asyncio.gather(
            *[
                self.client.spawn_task(
                    queue_name=self.queue_name,
                    task_name=f"claim-bench-{i}",
                    params={},
                )
                for i in range(num_tasks)
            ]
        )

        process = psutil.Process()
        start_mem = process.memory_info().rss / 1024 / 1024

        start = time.time()

        claimed = await asyncio.gather(
            *[self.client.claim_task(self.queue_name) for _ in range(num_tasks)],
            return_exceptions=True,
        )

        duration = time.time() - start

        end_mem = process.memory_info().rss / 1024 / 1024

        successful_claims = [c for c in claimed if not isinstance(c, Exception) and c is not None]
        errors = len(claimed) - len(successful_claims)

        result = BenchmarkResult(
            name="claim_throughput",
            num_tasks=num_tasks,
            duration_sec=duration,
            throughput=len(successful_claims) / duration,
            latency_p50=0,
            latency_p95=0,
            latency_p99=0,
            memory_used_mb=end_mem - start_mem,
            cpu_percent=0,
            errors=errors,
            success_rate=len(successful_claims) / num_tasks,
        )

        self.results.append(result)

        print(f"  :check_mark: Claimed {len(successful_claims)} tasks in {duration:.2f}s")
        print(f"  :check_mark: Throughput: {result.throughput:.1f} tasks/s")

        return result

    async def benchmark_complete_throughput(self, num_tasks: int = 1000):
        """Benchmark task completion throughput."""
        print(f"\n:white_heavy_check_mark: Benchmarking task completion ({num_tasks} tasks)...")

        # Spawn and claim tasks
        task_ids = await asyncio.gather(
            *[
                self.client.spawn_task(
                    queue_name=self.queue_name,
                    task_name=f"complete-bench-{i}",
                    params={},
                )
                for i in range(num_tasks)
            ]
        )

        claimed_tasks = []
        for _ in range(num_tasks):
            task = await self.client.claim_task(self.queue_name)
            if task:
                claimed_tasks.append(task["task_id"])

        start = time.time()

        results = await asyncio.gather(
            *[self.client.complete_task(task_id, {"result": "ok"}) for task_id in claimed_tasks],
            return_exceptions=True,
        )

        duration = time.time() - start

        errors = sum(1 for r in results if isinstance(r, Exception))
        success_count = len(results) - errors

        result = BenchmarkResult(
            name="complete_throughput",
            num_tasks=num_tasks,
            duration_sec=duration,
            throughput=success_count / duration,
            latency_p50=0,
            latency_p95=0,
            latency_p99=0,
            memory_used_mb=0,
            cpu_percent=0,
            errors=errors,
            success_rate=success_count / num_tasks,
        )

        self.results.append(result)

        print(f"  :check_mark: Completed {success_count} tasks in {duration:.2f}s")
        print(f"  :check_mark: Throughput: {result.throughput:.1f} tasks/s")

        return result

    async def benchmark_end_to_end_latency(self, num_tasks: int = 100):
        """Benchmark end-to-end latency (spawn :arrow_right: claim :arrow_right: complete)."""
        print(f"\n:stopwatch::variation_selector-16:  Benchmarking end-to-end latency ({num_tasks} tasks)...")

        latencies = []

        for i in range(num_tasks):
            start = time.time()

            # Spawn
            task_id = await self.client.spawn_task(
                queue_name=self.queue_name,
                task_name=f"latency-bench-{i}",
                params={},
            )

            # Claim
            claimed = await self.client.claim_task(self.queue_name)

            # Complete
            if claimed:
                await self.client.complete_task(claimed["task_id"], {})

            latency = time.time() - start
            latencies.append(latency)

        result = BenchmarkResult(
            name="end_to_end_latency",
            num_tasks=num_tasks,
            duration_sec=sum(latencies),
            throughput=num_tasks / sum(latencies),
            latency_p50=statistics.median(latencies),
            latency_p95=statistics.quantiles(latencies, n=20)[18],
            latency_p99=statistics.quantiles(latencies, n=100)[98],
            memory_used_mb=0,
            cpu_percent=0,
            errors=0,
            success_rate=1.0,
        )

        self.results.append(result)

        print(f"  :check_mark: P50 latency: {result.latency_p50*1000:.1f} ms")
        print(f"  :check_mark: P95 latency: {result.latency_p95*1000:.1f} ms")
        print(f"  :check_mark: P99 latency: {result.latency_p99*1000:.1f} ms")

        return result

    async def benchmark_concurrent_operations(self, num_concurrent: int = 50):
        """Benchmark concurrent operations from multiple clients."""
        print(f"\n:twisted_rightwards_arrows: Benchmarking concurrent operations ({num_concurrent} concurrent)...")

        async def worker_simulation(worker_id: int):
            """Simulate a worker performing operations."""
            latencies = []

            for i in range(10):
                start = time.time()

                # Spawn task
                task_id = await self.client.spawn_task(
                    queue_name=self.queue_name,
                    task_name=f"concurrent-{worker_id}-{i}",
                    params={},
                )

                # Claim task
                claimed = await self.client.claim_task(self.queue_name)

                # Complete task
                if claimed:
                    await self.client.complete_task(claimed["task_id"], {})

                latencies.append(time.time() - start)

            return latencies

        start = time.time()

        all_latencies = await asyncio.gather(*[worker_simulation(i) for i in range(num_concurrent)])

        duration = time.time() - start

        # Flatten latencies
        latencies = [lat for worker_lats in all_latencies for lat in worker_lats]

        total_tasks = len(latencies)

        result = BenchmarkResult(
            name="concurrent_operations",
            num_tasks=total_tasks,
            duration_sec=duration,
            throughput=total_tasks / duration,
            latency_p50=statistics.median(latencies),
            latency_p95=statistics.quantiles(latencies, n=20)[18],
            latency_p99=statistics.quantiles(latencies, n=100)[98],
            memory_used_mb=0,
            cpu_percent=0,
            errors=0,
            success_rate=1.0,
        )

        self.results.append(result)

        print(f"  :check_mark: {total_tasks} tasks from {num_concurrent} concurrent workers")
        print(f"  :check_mark: Throughput: {result.throughput:.1f} tasks/s")
        print(f"  :check_mark: P50 latency: {result.latency_p50*1000:.1f} ms")

        return result

    async def benchmark_queue_stats_performance(self):
        """Benchmark queue statistics query performance."""
        print(f"\n:bar_chart: Benchmarking queue stats performance...")

        # Create varied task states
        await asyncio.gather(
            *[self.client.spawn_task(self.queue_name, f"stats-task-{i}", {}) for i in range(500)]
        )

        # Claim and complete some
        for _ in range(100):
            claimed = await self.client.claim_task(self.queue_name)
            if claimed:
                await self.client.complete_task(claimed["task_id"], {})

        # Benchmark stats query
        latencies = []

        for _ in range(50):
            start = time.time()
            await self.client.get_queue_stats(self.queue_name)
            latencies.append(time.time() - start)

        result = BenchmarkResult(
            name="queue_stats",
            num_tasks=50,
            duration_sec=sum(latencies),
            throughput=50 / sum(latencies),
            latency_p50=statistics.median(latencies),
            latency_p95=statistics.quantiles(latencies, n=20)[18],
            latency_p99=statistics.quantiles(latencies, n=20)[19],
            memory_used_mb=0,
            cpu_percent=0,
            errors=0,
            success_rate=1.0,
        )

        self.results.append(result)

        print(f"  :check_mark: P50 latency: {result.latency_p50*1000:.1f} ms")
        print(f"  :check_mark: P95 latency: {result.latency_p95*1000:.1f} ms")

        return result

    def print_summary(self):
        """Print benchmark summary table."""
        print("\n" + "=" * 80)
        print("BENCHMARK SUMMARY")
        print("=" * 80)

        table_data = []
        for result in self.results:
            table_data.append(
                [
                    result.name,
                    result.num_tasks,
                    f"{result.duration_sec:.2f}s",
                    f"{result.throughput:.1f}",
                    f"{result.latency_p50*1000:.1f}ms" if result.latency_p50 > 0 else "N/A",
                    f"{result.latency_p95*1000:.1f}ms" if result.latency_p95 > 0 else "N/A",
                    f"{result.success_rate*100:.1f}%",
                ]
            )

        headers = [
            "Benchmark",
            "Tasks",
            "Duration",
            "Throughput\n(tasks/s)",
            "P50",
            "P95",
            "Success\nRate",
        ]
        print("\n" + tabulate(table_data, headers=headers, tablefmt="grid"))

    def save_results(self, output_path: Path):
        """Save benchmark results to JSON file."""
        data = {
            "timestamp": time.time(),
            "database_url": self.database_url,
            "queue_name": self.queue_name,
            "results": [asdict(r) for r in self.results],
        }

        with open(output_path, "w") as f:
            json.dump(data, f, indent=2)

        print(f"\n:floppy_disk: Results saved to: {output_path}")


async def main():
    """Run benchmark suite."""
    parser = argparse.ArgumentParser(description="Benchmark Absurd performance")
    parser.add_argument(
        "--database-url",
        default=os.getenv(
            "ABSURD_DATABASE_URL", "postgresql://postgres:postgres@localhost:5432/absurd"
        ),
        help="PostgreSQL connection URL",
    )
    parser.add_argument(
        "--queue-name",
        default=f"benchmark-{uuid4().hex[:8]}",
        help="Queue name for testing",
    )
    parser.add_argument(
        "--output",
        type=Path,
        default=Path("benchmark_results.json"),
        help="Output file for results",
    )
    parser.add_argument(
        "--quick",
        action="store_true",
        help="Run quick benchmark (reduced task counts)",
    )

    args = parser.parse_args()

    # Adjust task counts for quick mode
    if args.quick:
        spawn_tasks = 100
        claim_tasks = 100
        complete_tasks = 100
        latency_tasks = 50
        concurrent_workers = 10
    else:
        spawn_tasks = 1000
        claim_tasks = 1000
        complete_tasks = 1000
        latency_tasks = 100
        concurrent_workers = 50

    print("=" * 80)
    print("ABSURD PERFORMANCE BENCHMARK")
    print("=" * 80)
    print(f"Database: {args.database_url}")
    print(f"Queue: {args.queue_name}")
    print(f"Mode: {'Quick' if args.quick else 'Full'}")
    print("=" * 80)

    benchmark = AbsurdBenchmark(args.database_url, args.queue_name)

    try:
        await benchmark.setup()

        # Run benchmarks
        await benchmark.benchmark_spawn_throughput(spawn_tasks)
        await benchmark.benchmark_claim_throughput(claim_tasks)
        await benchmark.benchmark_complete_throughput(complete_tasks)
        await benchmark.benchmark_end_to_end_latency(latency_tasks)
        await benchmark.benchmark_concurrent_operations(concurrent_workers)
        await benchmark.benchmark_queue_stats_performance()

        # Print and save results
        benchmark.print_summary()
        benchmark.save_results(args.output)

    finally:
        await benchmark.teardown()


if __name__ == "__main__":
    asyncio.run(main())
